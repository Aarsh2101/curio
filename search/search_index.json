{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Curio: Collaborative Urban Insight Observatory","text":"<p>Welcome to Curio - a framework for collaborative urban visual analytics that uses a dataflow model with multiple abstraction levels (code, grammar, GUI elements) to facilitate collaboration across the design and implementation of visual analytics components. The framework allows experts to intertwine preprocessing, managing, and visualization stages while tracking provenance of code and visualizations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd04 Provenance-aware dataflow: Complete tracking and visualization of analysis history.</li> <li>\ud83d\udc65 Modularized and collaborative visual analytics: Work together across different stages.</li> <li>\ud83d\uddfa\ufe0f Support for 2D and 3D maps: Comprehensive geospatial visualization capabilities.</li> <li>\ud83d\udd17 Linked data-driven interactions: Interactive and connected visual components.</li> <li>\ud83e\udde9 Integration with UTK and Vega-Lite: Seamless integration with visualization frameworks.</li> </ul>"},{"location":"#whats-new-in-v05","title":"What's New in v0.5","text":"<p>Curio v0.5 introduces a number of improvements and fixes:</p> <ul> <li>\ud83d\udce6 Pip Installation Support: Curio can now be installed via <code>pip install utk-curio</code></li> <li>\ud83d\ude80 Performance Improvements: Enhanced computation execution speed in the backend</li> <li>\ud83e\uddea Initial End-to-End Testing: Integrated test for backend/sandbox testing</li> <li>\ud83e\udded New Examples Added: Included new dataflows like \"Complaints by Zip Code\" and \"Accessibility Analysis\"</li> <li>\ud83d\udc33 Docker Enhancements: Fixed Docker build issues and dependency installation errors</li> <li>\ud83e\uddf9 General Bug Fixes: Resolved issues with icons, route definitions, and upload status tracking</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>This project is part of the Urban Toolkit ecosystem, which includes Curio and UTK. Curio is a framework for collaborative urban visual analytics that uses a dataflow model with multiple abstraction levels to facilitate collaboration across the design and implementation of visual analytics components. UTK is a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban use cases in mind.</p> <p>Links: Paper | Website | Discord</p>      Your browser does not support the video tag."},{"location":"#citation","title":"Citation","text":"<p>Curio: A Dataflow-Based Framework for Collaborative Urban Visual Analytics Gustavo Moreira, Maryam Hosseini, Carolina Veiga, Lucas Alexandre, Nico Colaninno, Daniel de Oliveira, Nivan Ferreira, Marcos Lage, Fabio Miranda IEEE Transactions on Visualization and Computer Graphics (Volume: 31, Issue: 1, January 2025) Paper: [DOI] | [ArXiv]</p> <pre><code>@ARTICLE{moreira2025curio,\n  author={Moreira, Gustavo and Hosseini, Maryam and Veiga, Carolina and Alexandre, Lucas and Colaninno, Nicola and de Oliveira, Daniel and Ferreira, Nivan and Lage, Marcos and Miranda, Fabio},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Curio: A Dataflow-Based Framework for Collaborative Urban Visual Analytics}, \n  year={2025},\n  volume={31},\n  number={1},\n  pages={1224-1234},\n  doi={10.1109/TVCG.2024.3456353}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Curio and the Urban Toolkit have been supported by the National Science Foundation (NSF) (Awards #2320261, #2330565, and #2411223), Discovery Partners Institute (DPI), and IDOT.</p> <p>Ready to get started? Head over to the Installation Guide to begin!</p>"},{"location":"CONTRIBUTING/","title":"\ud83e\udd1d Contributing to Curio","text":"<p>Welcome to the Curio contributing guide! We're excited to collaborate on developing a framework for collaborative urban visual analytics that is both accessible and powerful. This guide is designed for students interested in contributing to open-source software, as well as developers looking to participate in the Curio ecosystem.</p> <p>New Contributors Welcome!</p> <p>Whether you're building your first pull request or integrating advanced features, this document is designed to support your contribution journey. Check out our GitHub Issues for good first tasks!</p> <p>About Curio</p> <p>Curio is actively evolving. Expect changes, and if you hit a snag, open a GitHub Issue\u2014we\u2019re here to help!</p>"},{"location":"CONTRIBUTING/#why-contribute","title":"\ud83c\udf1f Why Contribute","text":"<p>Contributing to Curio offers the opportunity to:</p> <ul> <li>\ud83d\ude80 Gain experience with a modern tech stack used in both research and industry</li> <li>\ud83d\udd2c Understand how visual analytics systems are built from the ground up  </li> <li>\ud83d\udc65 Collaborate with a team of researchers and urban analytics experts</li> <li>\ud83d\udcc1 Build a public portfolio of meaningful contributions (code, documentation, testing)</li> <li>\ud83c\udf0d Engage with real-world urban data: mobility, accessibility, environmental datasets</li> </ul>"},{"location":"CONTRIBUTING/#technology-overview","title":"\ud83d\udee0\ufe0f Technology Overview","text":"<p>Curio's architecture consists of multiple integrated components:</p> Component Technology Function Backend Python, Flask REST API for managing users, workflows, and provenance Frontend JavaScript, UTK, Vega-Lite Browser-based interface for authoring and interacting with dataflows Execution Python sandbox (multiprocess) Secure module for executing user code DevOps Docker, Docker Compose, GitHub Actions Containerization, deployment, and CI/CD Packaging PyPI (<code>utk-curio</code>) Distributes the CLI and backend/frontend bundle"},{"location":"CONTRIBUTING/#repository-structure","title":"\ud83d\udcc1 Repository Structure","text":"<p>The codebase follows a modular structure under the <code>utk_curio/</code> directory:</p> <pre><code>curio/\n\u251c\u2500\u2500 utk_curio/\n\u2502   \u251c\u2500\u2500 backend/                     # Manages database access and user authentication\n\u2502   \u2502   \u2514\u2500\u2500 tests/                   # pytest files for backend\n\u2502   \u251c\u2500\u2500 sandbox/                     # Executes user Python code in a secure environment\n\u2502   \u2502   \u2514\u2500\u2500 tests/                   # pytest files for sandbox\n\u2502   \u2514\u2500\u2500 frontend/                    # All frontend logic\n\u2502       \u251c\u2500\u2500 urban-workflows/         # Main Curio interface for dataflow editing\n\u2502       \u2502   \u2514\u2500\u2500 src/\n\u2502       \u2502       \u2514\u2500\u2500 components/      # React components and CSS\n\u2502       \u2514\u2500\u2500 utk-workflow/            # Embedded version of UTK\n\u2502\n\u251c\u2500\u2500 curio.py                         # CLI entry point for running and managing all services\n\u251c\u2500\u2500 examples/                        # Example dataflows and use cases\n\u251c\u2500\u2500 tests/                           # Dataflow examples for testing\n\u251c\u2500\u2500 docs/                            # Markdown documentation and usage guides\n\u251c\u2500\u2500 images/                          # Static images for documentation\n\u2514\u2500\u2500 requirements.txt                 # Backend and sandbox dependencies\n</code></pre>"},{"location":"CONTRIBUTING/#installation-options","title":"\ud83d\ude80 Installation Options","text":"<p>Choose the installation method that fits your contribution goals:</p> Via pip (Quick Setup)From GitHub (Development) <p>Perfect for: Testing and basic usage</p> <pre><code>pip install utk-curio\ncurio start\n</code></pre> <p>Frontend Limitations</p> <p>This installs the CLI and a pre-built version of the frontend. You won't be able to modify or rebuild the UI from this setup.</p> <p>Perfect for: Contributing code, developing features, and frontend modifications</p> <pre><code>git clone https://github.com/urban-toolkit/curio.git\ncd curio\npython curio.py start\n</code></pre> <p>Full Development Setup</p> <p>Refer to our Installation Guide for complete Docker instructions and frontend build steps.</p>"},{"location":"CONTRIBUTING/#suggested-contribution-paths","title":"\ud83c\udfaf Suggested Contribution Paths","text":"Contribution Area Specific Tasks \ud83e\uddea Testing and DebuggingImprove test coverage and reliability \u2022 Write or improve <code>pytest</code> tests in <code>backend/tests</code> and <code>sandbox/tests</code>\u2022 Reproduce and resolve issues from GitHub\u2022 Extend test coverage for edge cases\u2022 Test Curio across platforms (Windows, macOS, Linux) \ud83d\udd27 Developing Dataflow NodesExtend Curio's analytical capabilities \u2022 Add new analytic operations as reusable nodes\u2022 Improve UI and metadata descriptions \ud83d\udcca Example WorkflowsCreate learning resources for users \u2022 Create dataflow examples using public datasets\u2022 Annotate dataflows to serve as tutorials\u2022 Contribute to the <code>examples/</code> directory \ud83d\udcda DocumentationMake Curio more accessible \u2022 Write developer setup instructions or onboarding checklists\u2022 Add usage diagrams, screenshots, or schema explanations\u2022 Contribute inline documentation and docstrings\u2022 Improve API documentation \ud83c\udf10 Community and SupportHelp grow the Curio community \u2022 Suggest improvements to onboarding and usability\u2022 Help with community support on Discord\u2022 Create tutorials and learning resources"},{"location":"CONTRIBUTING/#getting-started-step-by-step","title":"\ud83c\udfc1 Getting Started (Step-by-Step)","text":""},{"location":"CONTRIBUTING/#1-fork-and-clone-the-repository","title":"1. Fork and Clone the Repository","text":"<p>Forking the Repository</p> <p>Visit the Curio GitHub page and click the \"Fork\" button in the upper-right corner. For more details, see GitHub's Forking a repo guide.</p> <p>After forking:</p> <pre><code># Clone your fork\ngit clone https://github.com/YOUR_USERNAME/curio.git\ncd curio\n\n# Set the original repository as upstream\ngit remote add upstream https://github.com/urban-toolkit/curio.git\n</code></pre>"},{"location":"CONTRIBUTING/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create conda environment (recommended)\nconda create -n curio python=3.10\nconda activate curio\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"CONTRIBUTING/#3-run-the-system","title":"3. Run the System","text":"<pre><code>python curio.py start\n</code></pre> <p>Development Server Running</p> <p>Open your browser and navigate to http://localhost:8080 to access the Curio interface.</p>"},{"location":"CONTRIBUTING/#4-create-a-feature-branch","title":"4. Create a Feature Branch","text":"<pre><code>git checkout -b my-feature\n</code></pre>"},{"location":"CONTRIBUTING/#5-make-changes-and-commit","title":"5. Make Changes and Commit","text":"<pre><code>git add .\ngit commit -m \"Add: feature description\"\ngit push origin my-feature\n</code></pre>"},{"location":"CONTRIBUTING/#6-submit-a-pull-request","title":"6. Submit a Pull Request","text":"<p>Open a PR on GitHub with a detailed description and link to relevant issues.</p>"},{"location":"CONTRIBUTING/#organizing-contributions","title":"\ud83d\udccb Organizing Contributions","text":""},{"location":"CONTRIBUTING/#defining-the-scope-of-a-pull-request","title":"Defining the Scope of a Pull Request","text":"<p>Focus Your PRs</p> <p>Each PR should ideally address a single feature or issue. Avoid mixing unrelated changes as it makes the review process harder and less transparent.</p> <p>Focus on: - \u2705 One feature addition - \u2705 One bug fix   - \u2705 One set of related documentation updates  </p> <p>If your PR grows beyond a single scope, consider splitting it into multiple PRs.</p>"},{"location":"CONTRIBUTING/#pull-request-template","title":"Pull Request Template","text":"<p>Use this template when creating a Pull Request:</p> <pre><code># Describe your changes\n\n\n# Issue resolved by this PR (if any)\n - Issue Number:\n - Link:\n\n# Type of change (Check all that apply)\n- [ ] Bug fix (non-breaking change which fixes an issue)\n- [ ] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n- [ ] Documentation Update\n- [ ] Other: \n\n# Parts of Curio impacted by this PR:\n- [ ] Frontend\n- [ ] Backend\n- [ ] Sandbox\n\n# Testing\n - [ ] Unit Tests\n - [ ] Manual Testing (please provide details below)\n\n# Screenshots (if relevant)\n\n\n# Checklist (Check all that apply)\n- [ ] I have manually loaded each .json test from the `tests/` folder into Curio, ran all the nodes one by one, and checked that they run without errors and give the expected results\n- [ ] I have commented my code, particularly in hard-to-understand areas\n- [ ] I have made corresponding changes to the documentation\n- [ ] My changes generate no new warnings\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] New and existing unit tests pass locally with my changes\n- [ ] Any dependent changes have been merged and published in downstream modules\n</code></pre>"},{"location":"CONTRIBUTING/#issue-template","title":"Issue Template","text":"<p>Before Creating Issues</p> <ul> <li>Check if the issue already exists</li> <li>Provide as much relevant detail as possible</li> </ul> <pre><code>### Summary\n&lt;!-- Provide a concise description of the issue. --&gt;\n\n### Steps to Reproduce\n&lt;!-- List the steps to replicate the problem --&gt;\n\n### Expected Result\n&lt;!-- What did you expect to happen? --&gt;\n\n### Actual Result\n&lt;!-- What actually happened? --&gt;\n\n### Environment\n&lt;!-- OS, Browser, Node version, Branch, etc. --&gt;\n\n### Additional Information\n&lt;!-- Screenshots, logs, temporary workarounds, etc. --&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#advice-for-students","title":"\ud83c\udf93 Advice for Students","text":"<p>Getting Started Tips</p> <ul> <li>Start small - improving documentation or examples is a valuable first step</li> <li>Ask questions early, especially if you're unfamiliar with the stack</li> <li>Use GitHub Issues to propose ideas and get feedback</li> <li>Consider pairing contributions with coursework or independent study</li> <li>Reach out for mentorship if you're committing to a larger contribution</li> </ul>"},{"location":"CONTRIBUTING/#development-resources","title":"Development Resources","text":"<ul> <li>\ud83d\udcd6 Installation Guide - Complete setup instructions</li> <li>\ud83d\ude80 Quick Start Tutorial - Learn the basics</li> <li>\ud83d\udcda User Guide - Detailed documentation</li> <li>\ud83c\udfaf Examples - Real-world use cases</li> </ul>"},{"location":"CONTRIBUTING/#final-notes","title":"\ud83c\udf89 Final Notes","text":"<p>Every contribution helps! You don't need deep expertise\u2014just curiosity, commitment, and a willingness to learn. Whether you're fixing a typo in documentation or implementing a new dataflow node, your work makes Curio better for the entire urban analytics community.</p> <p>Ready to contribute? Start by exploring our GitHub Issues and join our Discord community!</p> <p>Happy contributing! \ud83c\udf0d\u2728</p>"},{"location":"examples/examples/","title":"\ud83c\udf07 <code>Curio Examples</code> guide!","text":"<p>Have you walked through the Getting Started guide?</p> <p>Make sure to have walked through the Installation and Quick Start guide  before diving into these examples.</p> <p>The simplest approach to get to know more about how to work with <code>Curio</code> is to explore the  hands-on examples in this documentation. These step-by-step tutorials walk you through the framework's  features, from <code>visual analytics</code> and <code>data integration</code> to <code>what-if scenario planning</code> and <code>interactive visualizations</code>. </p> <p>Whether you are new to urban data science or an experienced researcher, these examples will help you unlock  <code>Curio</code>'s full potential for collaborative urban visual analytics.</p> <p>We use real open-source data throughout all of our examples, want to know where to get them?</p> <p>The public datasets used in these examples are available directly through the documentation. Each example  includes download links to the required datasets.</p> <ul> <li>Access datasets from examples:<ul> <li>Each tutorial includes direct download links to the required data files</li> <li>Data files are provided for immediate use with the examples</li> <li>Simply click the data links in each tutorial to download what you need</li> </ul> </li> <li>Alternative data sources:<ul> <li>Follow the original data source links provided in each tutorial</li> <li>Download datasets directly from their official channels (census data, city open data portals, etc.)</li> <li>Many examples use publicly available urban datasets from sources like OpenStreetMap and government APIs</li> </ul> </li> </ul> <p>Ready to start exploring urban data! \ud83c\udf89</p>"},{"location":"examples/examples/#curio-examples-explained","title":"<code>Curio</code> Examples Explained","text":"<p>The examples are organized into four main categories: <code>Visual Analytics</code>, <code>Urban Planning</code>,  <code>Advanced Analytics</code> and <code>Interactive Visualizations</code>. Here's an overview of what each tutorial covers:</p> <p>Icons indicate the complexity level of each example: \ud83d\udfe2 Easy, \ud83d\udfe1 Intermediate, \ud83d\udd34 Advanced.</p> \ud83d\udcca Visual Analytics &amp; Data Integration\ud83c\udfd7\ufe0f Urban Planning &amp; Scenarios\ud83e\udd16 Advanced Analytics &amp; Machine Learning\ud83d\udd17 Interactive Visualizations <p><code>Curio</code> excels at integrating heterogeneous urban datasets and creating compelling visual analytics workflows. The <code>Visual Analytics</code> section showcases how to combine different data types and create insightful visualizations.</p> <ul> <li> <p>\ud83d\udfe2 01 - Visual analytics of heterogeneous data: Learn how to integrate multiple urban datasets for thermal analysis.</p> <ul> <li>What it does: Integrates raster thermal data, meteorological readings, and sociodemographic data to compute the Universal Thermal Climate Index (UTCI) and analyze heat vulnerability across Milan neighborhoods.</li> </ul> </li> <li> <p>\ud83d\udfe2 09 - Building energy efficiency: Discover patterns in urban energy consumption.</p> <ul> <li>What it does: Compares mean and median energy use intensity across building types to identify outliers and efficiency gaps using interactive visualizations.</li> </ul> </li> <li> <p>\ud83d\udfe2 10 - Green roofs spatial analysis: Explore environmental infrastructure distribution.</p> <ul> <li>What it does: Visualizes the distribution and density of green roofs across Chicago using dot density maps and zip code aggregation techniques.</li> </ul> </li> </ul> <p><code>Curio</code> provides powerful tools for urban planning and scenario analysis, allowing planners to explore what-if scenarios and analyze accessibility patterns across urban environments.</p> <ul> <li> <p>\ud83d\udfe1 02 - What-if scenario planning: Explore the impact of urban development on shadow patterns.</p> <ul> <li>What it does: Creates interactive dataflows to simulate shadow impact from proposed buildings in Boston, allowing users to modify building heights and compare shadow patterns before and after changes.</li> </ul> </li> <li> <p>\ud83d\udfe2 04 - Accessibility analysis: Understand urban accessibility patterns.</p> <ul> <li>What it does: Analyzes sidewalk accessibility features using severity and agreement metrics to visualize neighborhood patterns and identify improvement areas.</li> </ul> </li> <li> <p>\ud83d\udfe2 05 - Flooding analysis: Map urban infrastructure complaints.</p> <ul> <li>What it does: Analyzes 311 flooding complaints to identify patterns in urban infrastructure issues, demonstrating data aggregation and spatial visualization techniques.</li> </ul> </li> </ul> <p><code>Curio</code> supports sophisticated machine learning workflows with human-in-the-loop capabilities, enabling experts to train, evaluate, and refine models collaboratively.</p> <ul> <li>\ud83d\udd34 03 - Expert-in-the-loop urban accessibility analysis: Combine AI with human expertise.<ul> <li>What it does: Demonstrates machine learning workflows for urban accessibility analysis, including model training, evaluation, and human-in-the-loop validation using computer vision for sidewalk assessment.</li> </ul> </li> </ul> <p><code>Curio</code> creates rich interactive experiences that allow users to explore urban data through multiple linked views and coordinated interactions between different visualization components.</p> <ul> <li> <p>\ud83d\udfe1 06 - Interactions between Vega-Lite and UTK: Connect maps and charts seamlessly.</p> <ul> <li>What it does: Demonstrates how to link user interactions between UTK map components and Vega-Lite plots for coordinated multi-view exploration.</li> </ul> </li> <li> <p>\ud83d\udfe1 07 - Speed camera violations: Analyze traffic patterns over time.</p> <ul> <li>What it does: Performs temporal aggregation and creates linked bar and line charts to analyze top camera violations over years with interactive filtering.</li> </ul> </li> <li> <p>\ud83d\udfe1 08 - Red-light traffic violation analysis: Build comprehensive traffic dashboards.</p> <ul> <li>What it does: Analyzes Chicago red-light violation data through multiple dataflows, creating seasonal trend analysis, camera effectiveness comparisons, and spatial distribution maps with coordinated interactions.</li> </ul> </li> <li> <p>\ud83d\udfe1 11 - Building energy consumption: Explore multi-dimensional energy data.</p> <ul> <li>What it does: Analyzes temporal, spatial, and structural patterns in building energy use with interactive visualizations supporting drill-down and comparison workflows. </li> </ul> </li> </ul> <p>Ready to dive deeper?</p> <p>Each example includes step-by-step instructions, code snippets, and downloadable datasets. For advanced features and detailed component documentation, explore the User Guide.</p> <p>User Guide </p>"},{"location":"examples/detailed_examples/01-visual-analytics/","title":"Example: Visual analytics of heterogeneous data","text":"<p>In this example, we will explore how Curio can facilitate visual  analytics of heterogeneous data by integrating various data sources such  as raster data, sensor data, and geospatial data to analyze and  visualize urban microclimate in Milan. Here is the overview of the  entire dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-1-load-high-resolution-mean-radiant-temperature-data","title":"Step 1: Load high-resolution mean radiant temperature data","text":"<p>The icons on the left-hand side can be used to instantiate different  nodes, including data loading nodes. Let\u2019s start by instantiating a data  loading node and changing its view to Code. Then, we load the  high-resolution mean radiant temperature data:</p> <pre><code>import rasterio\ntimestamp = 12\nsrc = rasterio.open(f'Milan_Tmrt_2022_203_{timestamp:02d}00D.tif')\n\nreturn src\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-2-loading-meteorological-data","title":"Step 2: Loading meteorological data","text":"<p>Using a Data loading /\u00a0 file node, we load air temperature (Td), wind  speed (Wind) and relative humidity (RH) data from ERA5 hourly  meteorological dataset.</p> <pre><code>import pandas as pd\nsensor = pd.read_csv('Milan_22.07.2022_Weather_File_UMEP_CSV.csv', delimiter=';')\nreturn sensor\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-25-merging-raster-and-meteorological-data","title":"Step 2.5: Merging raster and meteorological data","text":"<p>As an intermediate step, let\u2019s merge the dataflow from Step 1 and 2.</p> <p></p>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-3-compute-universal-thermal-climate-index-utci","title":"Step 3: Compute universal thermal climate index (UTCI)","text":"<p>In this step, we want to compute Universal Thermal Climate Index (UTCI)), a human biometeorology parameter to assess human well-being in the outdoor environment. The UTCI computation takes raster data as input, processes it, and produces another raster dataset as output. This output contains the UTCI values for each corresponding location in the grid.</p> <p>To do that, we connect the loaded data (raster and tabular) with a custom analysis &amp; modeling node that computes the UTCI. </p> <pre><code>import xarray as xr\nfrom pythermalcomfort import models\nimport numpy as np\nfrom rasterio.warp import Resampling\n\nsrc = arg[0]\nsensor = arg[1]\n\ntimestamp = 12\n\nupscale_factor = 0.25\ndataset = src\ndata = dataset.read(\n   out_shape=(\n       dataset.count,\n       int(dataset.height * upscale_factor),\n       int(dataset.width * upscale_factor)\n   ),\n   resampling=Resampling.nearest,\n   masked=True\n)\ndata.data[data.data==src.nodatavals[0]] = np.nan\n\nsensor = sensor[sensor['it']==timestamp]\ntdb = sensor['Td'].values[0]\nv = sensor['Wind'].values[0]\nrh = sensor['RH'].values[0]\n\ndef xutci(tdb, tr, v, rh, units='SI'):\n   return xr.apply_ufunc(\n       models.utci,\n       tdb,\n       tr,\n       v,\n       rh,\n       units\n   )\n\nutci = xutci(tdb, data[0], v, rh)\n\nreturn (utci.tolist(), [data.shape[-1], data.shape[-2]])\n</code></pre>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-4-loading-sociodemographic-data","title":"Step 4: Loading sociodemographic data","text":"<p>To study the relationship between UTCI and vulnerable populations, we create a new data node that loads sociodemographic data for populations older than 65 at neighborhood level.</p> <p></p> <pre><code>import geopandas as gpd\ngdf = gpd.read_file('R03_21-11_WGS84_P_SocioDemographics_MILANO_Selected.shp')\nreturn gdf\n</code></pre>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-5-merge-data","title":"Step 5: Merge data","text":"<p>Now, we want to spatially join the UTCI data in the raster format with the socio-demographic data loaded in the previous step. To do that, we create another analysis &amp; modeling node, and run the following:</p> <p></p> <pre><code>import numpy as np\nfrom rasterstats import zonal_stats\n\ndataset = arg[0]\nutci = np.array(arg[1][0])\nshape = arg[1][1]\ngdf = arg[2]\n\ntransform = dataset.transform * dataset.transform.scale(\n   (dataset.width / shape[0]),\n   (dataset.height / shape[1])\n)\n\njoined = zonal_stats(gdf, utci, stats=['min','max','mean','median'], affine=transform)\n\ngdf['mean'] = [d['mean'] for d in joined]\n\nreturn gdf.loc[:, [gdf.geometry.name, 'mean', \"gt_65\"]]\n</code></pre> <p>We then filter the resulting gdf to only those with mean UTCI higher than zero. Let\u2019s create a new data cleaning node connected to the previous node and store the result on a data node:</p> <pre><code>import geopandas as gpd\n\ngdf = arg\n\nfiltered_gdf = gdf.set_crs(32632)\nfiltered_gdf = filtered_gdf.to_crs(3395)\n\nfiltered_gdf = filtered_gdf[filtered_gdf['mean']&gt;0]\n\nfiltered_gdf.metadata = {\n   'name': 'census'\n}\n\nreturn filtered_gdf\n</code></pre>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-6-create-a-map-visualization","title":"Step 6: Create a map visualization","text":"<p>We can visualize the result of the previous operations by adding a UTK map. The grammar for the map is automatically populated once it receives an input from a previous box.</p> <p></p>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-7-create-a-linked-scatterplot","title":"Step 7: Create a linked scatterplot","text":"<p>In this step, we create a linked scatterplot through a Vega-Lite node connected to the output of the data node in Step 5.</p> <p> <pre><code>{\n \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n \"params\": [\n   {\"name\": \"clickSelect\", \"select\": \"interval\"}\n ],\n \"mark\": {\n   \"type\": \"point\",\n   \"cursor\": \"pointer\"\n },\n \"encoding\": {\n   \"x\": {\"field\": \"gt_65\", \"type\": \"quantitative\"},\n   \"y\": {\"field\": \"mean\", \"type\": \"quantitative\", \"scale\": {\"domain\": [37, 42]}},\n   \"fillOpacity\": {\n     \"condition\": {\"param\": \"clickSelect\", \"value\": 1},\n     \"value\": 0.3\n   },\n   \"color\": {\n     \"field\": \"interacted\",\n     \"type\": \"nominal\",\n     \"condition\": {\"test\": \"datum.interacted === '1'\", \"value\": \"red\", \"else\": \"blue\"}\n   }\n },\n \"config\": {\n   \"scale\": {\n     \"bandPaddingInner\": 0.2\n   }\n }\n}\n</code></pre></p>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-8-create-a-linked-boxplot","title":"Step 8: Create a linked boxplot","text":"<p>To create a box plot, we first create a \u201cData Cleaning\u201d node (connected to data node of Step 5) to filter out all attributes we are not interested and only keep the \u201cgreater than 65\u201d.</p> <p></p> <pre><code>gdf = arg\nreturn gdf.loc[:, [\"gt_65\"]]\n</code></pre> <p>Finally, we create a Vega-Lite node connected to the data cleaning node:</p> <pre><code>{\n \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n \"transform\": [\n   {\n     \"fold\": [\"gt_65\"],\n     \"as\": [\"Variable\", \"Value\"]\n   }\n ],\n \"mark\": {\n   \"type\": \"boxplot\",\n   \"size\": 60\n },\n \"encoding\": {\n   \"x\": {\"field\": \"Variable\", \"type\": \"nominal\", \"title\": \"Variable\"},\n   \"y\": {\"field\": \"Value\", \"type\": \"quantitative\", \"title\": \"Value\"}\n }\n}\n</code></pre>"},{"location":"examples/detailed_examples/01-visual-analytics/#step-9-link-map-and-scatterplot","title":"Step 9: Link map and scatterplot","text":"<p>The map, scatterplot, and boxplot are linked through interaction edges (red ones) connected to the data node, allowing for the analysis of outliers of concern, i.e., regions that have a large population of older adults and high UTCI.</p> <p></p>"},{"location":"examples/detailed_examples/01-visual-analytics/#final-result","title":"Final result","text":"<p>This example demonstrates how Curio can be used for visual analytics involving heterogeneous data sources. By integrating raster, tabular, and geospatial data, we can conduct comprehensive analyses of urban microclimate and visualize the results effectively. The linkage between different types of data and interactive visualization enables a deeper understanding of the relationships and potential areas of concern.</p>"},{"location":"examples/detailed_examples/02-what-if/","title":"Example: What-if scenarios with shadow data","text":"<p>In this example, we will explore how Curio can be used to create a dataflow that computes the shadow impact from proposed buildings in Boston. It is possible to compute the shadow impact of the proposed building over different times and seasons to support evidence-based environmental impact analysis. Here is the overview of the whole dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/02-what-if/#step-1-loading-physical-layers","title":"Step 1: Loading physical layers","text":"<p>We want our final map to have layers representing water, parks, and buildings. The icons on the left-hand side can be used to instantiate different nodes, including data loading nodes. Let\u2019s start by using UTK\u2019s Python API to download the necessary data. Change the node view to load and run the following code:</p> <pre><code># load box\nimport utk\n\nuc = utk.OSM.load([42.336844, -71.113459, 42.345559, -71.099216], layers=[{'name':'buildings', 'args': {'sizeCells': 5}}, {'name':'surface', 'args': {'sizeCells': 5}}, 'parks'])\n\n# buildings\ngdf_buildings = uc.layers['gdf']['sections'][0]\ngdf_buildings['thematic'] = 0.5\ngdf_buildings.metadata = {\n 'name': 'buildings'\n}\n\n#surface\njson_surface = uc.layers['json'][1]\ngdf_surface = uc.layers['gdf']['objects'][1]\ngdf_surface.metadata = {\n 'name': 'surface'\n}\n\n#parks\njson_parks = uc.layers['json'][2]\ngdf_parks = uc.layers['gdf']['objects'][2]\ngdf_parks.metadata = {\n 'name': 'parks'\n}\n\nreturn (json_surface, json_parks, gdf_buildings, gdf_surface, gdf_parks)\n</code></pre>"},{"location":"examples/detailed_examples/02-what-if/#step-15-getting-only-the-buildings","title":"Step 1.5: Getting only the buildings","text":"<p>From the composed output we want to get the GeoDataframe representing the buildings. That is why we need this intermediate step. Instantiate a \u201cComputation Analysis\u201d node, connect the output of the previous node to the input of the current and change the view to code and run the following code:</p> <pre><code>return arg[2]\n</code></pre> <p>Finally store the output in a data node.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-2-utk-for-interactions","title":"Step 2: UTK for interactions","text":"<p>Now we can add an UTK node instance that will receive the output of the previous data node and be used to change the height of the buildings. After connecting the nodes, change interaction mode to \u201cPICKING\u201d and run UTK.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-3-changing-building-height","title":"Step 3: Changing building height","text":"<p>Let\u2019s now create a \u201cData Transformation\u201d node and connect it to the same data node that feeds UTK in Step 2. To goal is to detect which buildings were interacted with and change its height. Change the view to \u201cCode\u201d and execute:</p> <pre><code>gdf = arg\n\ngdf.loc[gdf['interacted'] == '1', 'height'] *= [!! Height Multiplier$INPUT_VALUE$14 !!]\n\ngdf.metadata = {\n 'name': 'buildings'\n}\n\nreturn gdf\n</code></pre> <p>The Marker <code>[!! Height Multiplier$INPUT_VALUE$14 !!]</code> exposes a widget to control a height multiplier.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-35-merging-flows","title":"Step 3.5: Merging flows","text":"<p>As an intermediate step we will use a \u201cMerge\u201d node to unify the flow from Step 1 and Step 3.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#shadow-simulation-after-height-modification","title":"Shadow simulation (after height modification)","text":"<p>We can now add another \u201cComputation Analysis\u201d node to do shadow simulation based on the buildings Dataframe. To do that we connect the node to the merge of Step 3.5. Bear in mind that a NVIDIA GPU is needed. And execute the following code:</p> <pre><code>import utk\n\njson_surface = arg[0][0]\ngdf_surface = arg[0][3]\njson_parks = arg[0][1]\ngdf_parks = arg[0][4]\n\ngdf_buildings = arg[1]\n\njson_layers = [json_surface]\n# buildings json layer\ngdf_buildings = gdf_buildings.set_crs('4326')\nmesh = utk.OSM.mesh_from_buildings_gdf(gdf_buildings, 5)['data']\n\njson_buildings = {\n   'id': 'buildings',\n   'type': 'BUILDINGS_LAYER',\n   'renderStyle': ['SMOOTH_COLOR_MAP_TEX'],\n   'styleKey': 'building',\n   'data': mesh\n}\n\njson_layers.append(json_buildings)\n\nshadow = utk.data.shadow(json_layers, [[[!! Start date$INPUT_TEXT$12/26/2015 10:00 !!], [!! End date$INPUT_TEXT$12/26/2015 16:01 !!]]])\n\nthematic_layers = shadow.get_shadow_by_layer()\n\nbuilding_index = -1\ncurrent_building_id = -1\n\nvalues_per_row = []\n\nfor index, row in gdf_buildings.iterrows():\n   if(row['building_id'] != current_building_id):\n       current_building_id = row['building_id']\n       building_index += 1\n\n   values_per_row.append(thematic_layers['shadow0_buildings']['values'][building_index])\n\ngdf_buildings[\"shadow0_buildings\"] = values_per_row\n\ngdf_buildings.metadata = {\n 'name': 'buildings'\n}\n\nvalues_per_row = []\n\nfor index, row in gdf_surface.iterrows():\n   values_per_row.append(thematic_layers['shadow0_surface']['values'][index])\n\ngdf_surface[\"shadow0_surface\"] = values_per_row\ngdf_surface[\"surface_id\"] = 0 # surface is a single big bounding box\n\ngdf_surface.metadata = {\n 'name': 'surface'\n}\n\nreturn (gdf_surface, gdf_parks, gdf_buildings)\n</code></pre> <p>The Markers <code>[!! Start date$INPUT_TEXT$12/26/2015 10:00 !!]</code> and <code>[!! End date$INPUT_TEXT$12/26/2015 16:01 !!]</code> expose a widget to control the start and end date on the shadow simulation.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-5-visualizing-shadows","title":"Step 5: Visualizing shadows","text":"<p>To visualize the result of the simulation we can add an UTK node and connect to the output of Step 4. Don\u2019t forget to run the UTK node.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-6-shadow-simulation-before-height-modification","title":"Step 6: Shadow simulation (before height modification)","text":"<p>Now, we repeat what was done in Step 4 but to simulate the shadows for the dataset before the height modification. To do that, add a \u201cComputation Analysis\u201d node, connect it to the output of Step 1, and execute the following code:</p> <pre><code>import utk\n\njson_surface = arg[0]\njson_parks = arg[1]\ngdf_buildings = arg[2]\ngdf_surface = arg[3]\ngdf_parks = arg[4]\n\njson_layers = [json_surface]\n\n# buildings json layer\ngdf_buildings = gdf_buildings.set_crs('4326')\nmesh = utk.OSM.mesh_from_buildings_gdf(gdf_buildings, 5)['data']\n\njson_buildings = {\n   'id': 'buildings',\n   'type': 'BUILDINGS_LAYER',\n   'renderStyle': ['SMOOTH_COLOR_MAP_TEX'],\n   'styleKey': 'building',\n   'data': mesh\n}\n\njson_layers.append(json_buildings)\n\nshadow = utk.data.shadow(json_layers, [[[!! Start date$INPUT_TEXT$12/26/2015 10:00 !!], [!! End date$INPUT_TEXT$12/26/2015 16:01 !!]]])\n\nthematic_layers = shadow.get_shadow_by_layer()\n\nbuilding_index = -1\ncurrent_building_id = -1\n\nvalues_per_row = []\n\nfor index, row in gdf_buildings.iterrows():\n   if(row['building_id'] != current_building_id):\n       current_building_id = row['building_id']\n       building_index += 1\n\n   values_per_row.append(thematic_layers['shadow0_buildings']['values'][building_index])\n\ngdf_buildings[\"shadow0_buildings\"] = values_per_row\n\ngdf_buildings.metadata = {\n 'name': 'buildings'\n}\n\nvalues_per_row = []\n\nfor index, row in gdf_surface.iterrows():\n   values_per_row.append(thematic_layers['shadow0_surface']['values'][index])\n\ngdf_surface[\"shadow0_surface\"] = values_per_row\ngdf_surface[\"surface_id\"] = 0 # surface is a single big bounding box\n\ngdf_surface.metadata = {\n 'name': 'surface'\n}\n\nreturn (gdf_surface, gdf_parks, gdf_buildings)\n</code></pre>"},{"location":"examples/detailed_examples/02-what-if/#step-7-visualizing-shadows","title":"Step 7: Visualizing shadows","text":"<p>Similar to Step 5, create a UTK node to visualize the shadows simulated on Step 6.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-75-merging-the-what-if-flow","title":"Step 7.5: Merging the what-if flow","text":"<p>To see the difference between the shadows calculated on Step 4 and 6 we have to merge the data flow of Steps 5 and 7.</p> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-8-what-if-scenario","title":"Step 8: What-if scenario","text":"<p>To create the what-if scenario we can calculate the difference between the shadow calculated for the buildings before and after modifying the heights. Create a \u201cComputation Analysis\u201d node connected to the output of Step 7.5 and execute the following code:</p> <pre><code>gdf_surface_1 = arg[0][0]\ngdf_buildings_1 = arg[0][2]\n\ngdf_surface_2 = arg[1][0]\ngdf_buildings_2 = arg[1][2]\n\ngdf_parks_3 = arg[0][1]\n\nlist_1 = gdf_surface_1.iloc[0]['shadow0_surface']\nlist_2 = gdf_surface_2.iloc[0]['shadow0_surface']\n\ndifference_list = [b - a for a, b in zip(list_1, list_2)]\n\ngdf_surface_2['shadow0_surface'] = [difference_list]\n\ngdf_parks_3.metadata = {\n 'name': 'parks'\n}\n\ngdf_surface_2.metadata = {\n 'name': 'surface'\n}\n\ngdf_buildings_2.metadata = {\n 'name': 'buildings'\n}\n\nreturn (gdf_parks_3, gdf_surface_2, gdf_buildings_2)\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/02-what-if/#step-9-visualizing-what-if-scenario","title":"Step 9: Visualizing what-if scenario","text":"<p>Finally, to visualize the result, we can add a UTK node connected to Step 8.</p> <p></p>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/","title":"Example: Expert-in-the-loop urban accessibility analyses","text":"<p>In this example, we are going to learn how Curio can facilitate expert-in-the-loop inspection of a computer vision model for sidewalk surface material classification. Here is the overview of the whole dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-0-initializing-curio","title":"Step 0: Initializing Curio","text":"<p>In order to run this tutorial, make sure you satisfy the requirements from CitySurfaces.</p> <p>After initializing Curio, you will see a blank canvas.</p>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-1-loading-the-model-training-node","title":"Step  1: Loading the model training node","text":"<p>The icons on the left-hand side of the interface can be used to instantiate different nodes, including analysis &amp; modeling nodes. Let\u2019s start by instantiating an Analysis &amp; Modeling node and changing its view to Code. Then, we set up the training procedure for our model.</p> <p>For simplicity, we are not loading the exact segmentation model used in CitySurfaces since it is a resource-intensive model. Instead, we use a lighter version for demonstration purposes:</p> <p>After hitting run, the Python return will output train_model() for the next node. Curio\u2019s provenance feature allows the expert to analyze several versions of their training procedure.</p> <pre><code># computation analysis - clear\n\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.pytorch import ToTensorV2\nfrom cityscapesscripts.helpers.labels import trainId2label as t2l\nimport segmentation_models_pytorch as smp\nfrom torch import nn, optim\n\nimport albumentations as A  \nimport torch\nimport glob\nimport numpy as np\n\nIMG_DIR = './dataset/city-surfaces'\nIMAGE_WIDTH   = 320  \nIMAGE_HEIGHT  = 320\nBATCH_SIZE    = 8\nNUM_CLASSES   = 3 # 10\nLEARNING_RATE = 0.002\nDEVICE  = \"cuda\" # if torch.cuda.is_available() else \"cpu\"\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, img_dir, transform=None):\n        self.img_dir  = img_dir\n        self.transform  = transform\n        self.images = glob.glob('%s/*.png'%(self.img_dir))\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        img_path    = self.images[index]\n        mask_path   = self.images[index].replace('images','annotations')\n        image       = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.float32) / 255.0\n        y           = np.array(Image.open(mask_path).convert(\"L\"))\n        y = y - 1\n        y[y==0]=0 # concrete\n        y[y==1]=0 # bricks\n        y[y==2]=0 # granite\n        y[y==3]=0 # asphalt\n        y[y==4]=0 # mixed\n        y[y==5]=1 # road\n        y[y==6]=2 # background\n        y[y==7]=0\n        y[y==8]=0\n        y[y==9]=0\n\n        if self.transform is not None:\n            augmentations = self.transform(image=image, mask=y)\n            image   = augmentations[\"image\"].to(torch.float32)\n            y       = augmentations[\"mask\"].type(torch.LongTensor)\n\n        return image, y\n\ntrain_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.ColorJitter(p=0.2),\n        A.HorizontalFlip(p=0.5),\n        ToTensorV2(),\n    ],\n)\n\nval_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        ToTensorV2(),\n    ],\n)\n\ndef get_loaders(img_dir, batch_size, train_transform, val_transform):\n\n    train_ds     = SegmentationDataset(IMG_DIR+'//train//images//' , transform=train_transform)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n\n    val_ds       = SegmentationDataset(IMG_DIR+'//val//images//', transform=val_transform)\n\n    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n\n    return train_loader, val_loader\n\ntrain_loader, val_loader = get_loaders(IMG_DIR, BATCH_SIZE, train_transform, val_transform)\n\ndef check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels  = 0\n    dice_score  = 0\n    iou_score   = 0\n    model.eval()\n\n    with torch.no_grad():\n        for image, mask in loader:\n            image = image.to(device)\n            mask  = mask.to(device)\n            predictions = model(image)\n            pred_labels = torch.argmax(predictions, dim=1)\n\n            cpred_labels = pred_labels.cpu().detach().numpy()\n            cmask = mask.cpu().detach().numpy()\n            cciou_score = 0\n            ccnum_correct = 0\n            ccnum_pixels = 0\n            intersection_per_class = np.zeros(NUM_CLASSES)\n            union_per_class = np.zeros(NUM_CLASSES)\n            for class_idx in range(NUM_CLASSES):\n                ccpred_labels = cpred_labels==class_idx\n                ccmask = cmask==class_idx\n\n                intersection = np.logical_and(ccpred_labels, ccmask)\n                union = np.logical_or(ccpred_labels, ccmask)\n                intersection_per_class[class_idx] = np.sum(intersection)\n                union_per_class[class_idx] = np.sum(union)\n\n            iou_per_class = intersection_per_class / (union_per_class + 1e-10)\n            iou_score += np.mean(iou_per_class)\n    model.train()\n\n\nmodel = smp.Unet(encoder_name='efficientnet-b3', in_channels=3, classes=NUM_CLASSES, activation='softmax2d').to(DEVICE)\nloss_fn = nn.CrossEntropyLoss(ignore_index=255)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nNUM_EPOCHS    = 10\n\ndef train_fn(loader, model, optimizer, loss_fn):\n\n    for batch_idx, (image, mask) in enumerate(loader):\n        image   = image.to(device=DEVICE)\n        mask    = mask.to(device=DEVICE)\n\n        # forward\n        predictions = model(image)\n        loss = loss_fn(predictions, mask)\n\n        # backward\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n\nfor epoch in range(NUM_EPOCHS):\n\n    train_fn(train_loader, model, optimizer, loss_fn)\n\n    # check accuracy\n    check_accuracy(val_loader, model, device=DEVICE)\n    # break\n\ntorch.save(model.state_dict(), 'model.pth')\n\nreturn \"Model saved in model.pth\"\n</code></pre>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-2-creating-the-boston-physical-layer","title":"Step 2: Creating the Boston physical layer","text":"<p>Next, we create a Data Loading node and change its view to Code. We load a sample of 100 unlabeled, unseen images.</p> <pre><code>python\nimport pandas as pd\ndf = pd.read_csv('./dataset/gsv/boston_gsv../data/.csv', names=['status','id','lat','lon'])\nsample = df[df['status']=='OK'].sample(100, random_state=42)\nreturn sample\n</code></pre>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-3-computing-prediction-uncertainty","title":"Step 3: Computing prediction uncertainty","text":"<p>Now, we create an Analysis &amp; Modeling node to calculate the prediction uncertainty of the model on the new set of unseen data and connect it to the previous node. The goal is to measure the difference between the two highest prediction probabilities in the softmax layer.</p> <p></p> <pre><code>import torch\nimport segmentation_models_pytorch as smp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\n\nsample = arg\n\ndef compute_uncertainty(predictions):\n   sorted_probs = np.sort(predictions, axis=1)\n   highest_prob = sorted_probs[:, -1, :, :]  # Highest probability for each pixel\n   second_highest_prob = sorted_probs[:, -2, :, :]  # Second highest probability\n   uncertainty_margin = highest_prob - second_highest_prob\n   return 1.0-uncertainty_margin\n\nmodel = smp.Unet(encoder_name='efficientnet-b3', in_channels=3, classes=6, activation='softmax2d').to('cuda')\nmodel.load_state_dict(torch.load('model.pth'))\n\ncolor_map = {\n   0: (68, 1, 84, 255),\n   1: (64, 67, 135, 255),\n   2: (41, 120, 142, 255),\n   3: (34, 167, 132, 255),\n   4: (121, 209, 81, 255),\n   5: (253, 231, 36, 255),\n}\n\nlats = []\nlons = []\nuncerts = []\nimages = []\npredicted_images = []\nuncert_images = []\nfor index, row in sample.iterrows():\n   image_path = image_path = './dataset/gsv/boston/%s_left.jpg'%row['id']\n\n   pil_image = Image.open(image_path).convert(\"RGB\").resize((320,320))\n\n   image = np.array(pil_image, dtype=np.float32) / 255.0\n   predictions = model(torch.from_numpy(image.reshape(1,320,320,3)).permute((0,3,1,2)).to('cuda'))\n\n   pred_labels = torch.argmax(predictions, dim=1)\n   pred_array = pred_labels.cpu().numpy()\n   pred_array = pred_array.reshape((320, 320))\n   pred_pil = Image.new(\"RGB\", (pred_array.shape[1], pred_array.shape[0]))\n   for i in range(pred_array.shape[0]):\n       for j in range(pred_array.shape[1]):\n           pred_pil.putpixel((j, i), color_map[pred_array[i, j]])\n\n   # pred_array = np.uint8((pred_array/2) * 255)\n   # pred_array = np.transpose(pred_array, (1, 2, 0))\n   # pred_array = np.squeeze(pred_array, axis=2)\n   # pred_pil = Image.fromarray(pred_array)\n\n   buffered = BytesIO()\n   pred_pil.save(buffered, format=\"PNG\")\n   pred_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n   uncertainty_margin = compute_uncertainty(predictions.cpu().detach().numpy())\n\n   uncertainty_array = np.uint8(uncertainty_margin * 255)\n   uncertainty_array = np.transpose(uncertainty_array, (1, 2, 0))\n   uncertainty_array = np.squeeze(uncertainty_array, axis=2)\n   uncertainty_pil = Image.fromarray(uncertainty_array)\n\n   buffered = BytesIO()\n   uncertainty_pil.save(buffered, format=\"PNG\")\n   uncertainty_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n   lats.append(row['lat'])\n   lons.append(row['lon'])\n   uncerts.append(float(np.average(uncertainty_margin)))\n\n   buffered = BytesIO()\n   pil_image.save(buffered, format=\"PNG\")\n   img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n   images.append(img_str)\n   predicted_images.append(pred_str)\n   uncert_images.append(uncertainty_str)\n\nreturn (lats, lons, uncerts, images, predicted_images, uncert_images)\n</code></pre> <p>Now, let\u2019s connect this node to a new computation analysis node and create a GeoDataFrame and connect it to a data node, where each image and its associated uncertainty will be represented as a geospatial point feature.</p> <pre><code>import geopandas as gpd\n\nlats = arg[0]\nlons = arg[1]\nuncerts = arg[2]\noriginal_images = arg[3]\npredicted_images = arg[4]\nuncert_images = arg[5]\n\nimage_content = list(zip(original_images, predicted_images, uncert_images))\n\ngdf = pd.DataFrame({'lat': lats, 'lon': lons, 'uncertainty': uncerts, 'image_content': image_content})\n\ngdf['image_id'] = gdf.index\n\ngdf = gpd.GeoDataFrame(\n   gdf, geometry=gpd.points_from_xy(gdf.lon, gdf.lat), crs=\"EPSG:4326\"\n)\n\ngdf = gdf.sort_values(by='image_id', ascending=True)\n\nreturn gdf\n</code></pre>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-4-filtering-most-uncertain-images","title":"Step 4: Filtering most uncertain images","text":"<p>We then filter the most uncertain images by connecting a \u201cComputation Analysis\u201d node to the previous data node.</p> <pre><code>df = pd.DataFrame(arg.drop(columns=arg.geometry.name))\ndf = df[df['interacted'] == '1']\ndf = df.sort_values(by='uncertainty', ascending=False)\nreturn df.head(20)\n</code></pre>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-5-visualizing-the-images","title":"Step 5: Visualizing the images","text":"<p>Finally, we visualize the images by simply adding an image node.</p> <p></p>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-6-loading-neighborhood-data","title":"Step 6: Loading neighborhood data","text":"<p>Next, we create a Data Loading node and change its view to Code. We load the physical layer describing neighborhoods in Boston:</p> <pre><code>import geopandas as gpd\n# Load neighborhood data\nboston = gpd.read_file('Census2020_BlockGroups../data/.shp').to_crs('EPSG:4326')\nreturn boston\n</code></pre>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-7-merging-the-data","title":"Step 7: Merging the data","text":"<p>We now merge the uncertainty data from Step 5 with the neighborhood data in Step 3, to help us determine the optimal neighborhood from which to sample our next set of images. </p> <p></p> <p>To do that, we create a new computation analysis node, and change its view to code, and run the following:</p> <pre><code>import geopandas as gpd\n\nboston = arg[0]\ngdf = arg[1]\n\ndef agg_to_list(series):\n   return list(series)\n\njoined = gpd.sjoin(boston, gdf).groupby('GEOID20').agg({'uncertainty': 'mean', 'image_id': agg_to_list})\nboston = boston.set_index('GEOID20')\nboston.loc[joined.index,'uncertainty'] = joined['uncertainty']\nboston.loc[joined.index,'image_id'] = joined['image_id']\n\nfiltered_boston = boston.loc[joined.index]\n\nfiltered_boston = filtered_boston.rename(columns={'image_id': 'linked'})\n\nreturn filtered_boston\n</code></pre> <p>Now, let\u2019s clean the <code>filtered_boston</code> GeoDataFrame by creating a new data cleaning node and connecting it to the previous one. </p> <pre><code>import geopandas as gpd\n\nfiltered_boston = arg\n\nfiltered_boston = filtered_boston.loc[:, [filtered_boston.geometry.name, 'uncertainty', 'linked']]\n\nfiltered_boston = filtered_boston.set_crs(4326)\nfiltered_boston = filtered_boston.to_crs(3395)\n\nfiltered_boston.metadata = {\n   'name': 'boston'\n}\n\nreturn filtered_boston\n</code></pre>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-8-visualizing-prediction-uncertainty","title":"Step 8: Visualizing prediction uncertainty","text":"<p>In this step, we want to create a spatial map showing the distribution of prediction uncertainties over neighborhoods of Boston.</p> <p></p> <p>To achieve that, let\u2019s create a data node and a UTK visualization node connected to the neighborhood data node. UTK\u2019s grammar is automatically populated once an input is received.</p>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#step-9-analyzing-and-identifying-shortcomings","title":"Step 9: Analyzing and identifying shortcomings","text":"<p>These nodes are then used to identify potential shortcomings with the model that require new labeled data. The sorted mosaic of images helps identify patterns of failures where the model had the most difficulty classifying. This signals the need for sampling more images with similar light/shadow and built environment conditions:</p> <p></p>"},{"location":"examples/detailed_examples/03-expert-in-the-loop/#final-result","title":"Final result","text":"<p>The final visualization shows the prediction uncertainties overlaid on the map of Boston\u2019s neighborhoods, facilitating a more targeted approach to image labeling. Given that dense labeling of images is an expensive endeavor, Curio facilitates a more targeted approach, in which conditions are identified and can be used as a guide for labeling.</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/","title":"Example: Visual Analytics of Sidewalk Accessibility Data","text":"<p>Author: makaveli2P</p> <p>In this example, we will explore how Curio can be used to analyze and visualize sidewalk accessibility data from Project Sidewalk. This pipeline enables urban planners and accessibility advocates to identify accessibility issues, prioritize areas for improvement, and make data-driven decisions. Here is the overview of the entire dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio's main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here. Download the Shapefile of the Access Attributes with labels. </p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/#step-1-load-sidewalk-accessibility-data","title":"Step 1: Load sidewalk accessibility data","text":"<p>The icons on the left-hand side can be used to instantiate different nodes. Let's start by creating a \"Data Loading\" node to import the shapefile data containing accessibility features:</p> <pre><code>import geopandas as gpd\n\ngdf = gpd.read_file('attributes_2025-02-26-08/52/11.0../data/.shp')\n\ngdf.metadata = {\n    'name': 'accessibility_features'\n}\n\nreturn gdf\n</code></pre> <p></p> <p>This node loads geospatial data containing information about different types of sidewalk accessibility features, such as curb ramps, missing curb ramps, obstacles, and surface problems. The data includes attributes like feature type, severity, neighborhood, and geometry.</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/#step-2-data-cleaning-and-processing","title":"Step 2: Data cleaning and processing","text":"<p>Now, let's create a \"Data Cleaning\" node and connect it to the output of Step 1. This node will prepare our data for visualization by: 1. Selecting relevant columns 2. Calculating agreement metrics 3. Categorizing severity levels 4. Adding necessary information for UTK visualization</p> <pre><code>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport utk\n\ngdf = arg\n\nprocessed_gdf = gdf[['labelType', 'severity', 'neighborhd', 'geometry', 'nAgree', 'nDisagree']]\n\nprocessed_gdf['agreement_ratio'] = processed_gdf['nAgree'] / (processed_gdf['nAgree'] + processed_gdf['nDisagree'])\n\nseverity_bins = [0, 1, 2, 3, 5]\nseverity_labels = ['Low', 'Medium', 'High', 'Critical']\nprocessed_gdf['severity_level'] = pd.cut(\n    processed_gdf['severity'],\n    bins=severity_bins,\n    labels=severity_labels,\n    include_lowest=True\n)\n\nif processed_gdf.crs is None:\n    processed_gdf = processed_gdf.set_crs(\"EPSG:4326\")\nelse:\n    processed_gdf = processed_gdf.to_crs(\"EPSG:4326\")\n\nprocessed_gdf['thematic'] = processed_gdf['severity']\n\nprocessed_gdf.metadata = {\n    'name': 'accessibility_analysis'\n}\n\nreturn processed_gdf\n</code></pre> <p>This cleaning step is essential because: - It extracts only the relevant attributes for our analysis - The agreement ratio helps us understand the consensus about each feature - Severity categories make it easier to visualize different levels of accessibility issues - The correct coordinate reference system ensures proper geospatial visualization - The thematic value enables color coding in UTK visualizations</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/#step-3-feature-type-analysis","title":"Step 3: Feature type analysis","text":"<p>To understand the distribution of accessibility issues by type, we'll create an \"Data Transformation\" node connected to the output of Step 2:</p> <pre><code>import pandas as pd\nimport numpy as np\n\ngdf = arg\n\nfeature_stats = gdf.groupby('labelType').agg(\n    count=('labelType', 'count'),\n    avg_severity=('severity', 'mean'),\n    avg_agreement=('agreement_ratio', 'mean')\n).reset_index()\n\nfeature_stats = feature_stats.fillna(0)\n\nfor col in ['avg_severity', 'avg_agreement']:\n    feature_stats[col] = feature_stats[col].astype(float)\n\nfeature_stats.metadata = {\n    'name': 'feature_stats'\n}\n\nreturn feature_stats\n</code></pre> <p>This analysis aggregates the data by feature type to help us: - Identify which types of accessibility issues are most common - Compare the average severity across different feature types - Understand the level of agreement about different features</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/#step-4-neighborhood-analysis","title":"Step 4: Neighborhood analysis","text":"<p>To compare accessibility issues across different neighborhoods, we'll create another \"Data Transformation\" node connected to the output of Step 2:</p> <pre><code>import pandas as pd\nimport numpy as np\n\ngdf = arg\n\nneighborhood_stats = gdf.groupby('neighborhd').agg(\n    count=('labelType', 'count'),\n    avg_severity=('severity', 'mean'),\n    avg_agreement=('agreement_ratio', 'mean')\n).reset_index()\n\nneighborhood_stats = neighborhood_stats.fillna(0)\n\nfor col in ['avg_severity', 'avg_agreement']:\n    neighborhood_stats[col] = neighborhood_stats[col].astype(float)\n\nneighborhood_stats.metadata = {\n    'name': 'neighborhood_stats'\n}\n\nreturn neighborhood_stats\n</code></pre> <p>This analysis is crucial because it helps urban planners: - Identify neighborhoods with the highest concentration of accessibility issues - Compare the severity of issues across different areas - Prioritize neighborhoods for accessibility improvements</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/#step-5-feature-type-distribution-visualization","title":"Step 5: Feature type distribution visualization","text":"<p>To complement the spatial visualization, let's create a \"2D Plot (Vega Lite)\" node connected to the output of Step 3 (feature_stats):</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": {\"name\": \"feature_stats\"},\n  \"mark\": \"bar\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"labelType\",\n      \"type\": \"nominal\",\n      \"title\": \"Feature Type\"\n    },\n    \"y\": {\n      \"field\": \"count\",\n      \"type\": \"quantitative\",\n      \"title\": \"Number of Features\"\n    },\n    \"color\": {\n      \"field\": \"avg_severity\",\n      \"type\": \"quantitative\",\n      \"title\": \"Average Severity\",\n      \"scale\": {\n        \"scheme\": \"viridis\"\n      }\n    },\n    \"tooltip\": [\n      {\"field\": \"labelType\", \"type\": \"nominal\", \"title\": \"Feature Type\"},\n      {\"field\": \"count\", \"type\": \"quantitative\", \"title\": \"Count\"},\n      {\"field\": \"avg_severity\", \"type\": \"quantitative\", \"title\": \"Avg Severity\", \"format\": \".2f\"},\n      {\"field\": \"avg_agreement\", \"type\": \"quantitative\", \"title\": \"Avg Agreement\", \"format\": \".2f\"}\n    ]\n  }\n}\n</code></pre> <p></p> <p>This bar chart visualization provides a clear view of: - The distribution of different types of accessibility issues - A comparison of their frequencies - The average severity of each type (through color coding) - Additional metrics through tooltips for interactive exploration</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/#step-6-neighborhood-comparison-visualization","title":"Step 6: Neighborhood comparison visualization","text":"<p>Let's create another \"2D Plot (Vega Lite)\" node connected to the output of Step 4 (neighborhood_stats):</p> <p><pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": {\"name\": \"neighborhood_stats\"},\n  \"mark\": \"circle\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"neighborhd\",\n      \"type\": \"nominal\",\n      \"title\": \"Neighborhood\"\n    },\n    \"y\": {\n      \"field\": \"count\",\n      \"type\": \"quantitative\",\n      \"title\": \"Number of Features\"\n    },\n    \"size\": {\n      \"field\": \"count\",\n      \"type\": \"quantitative\",\n      \"title\": \"Number of Features\",\n      \"scale\": {\n        \"range\": [50, 500]\n      }\n    },\n    \"color\": {\n      \"field\": \"avg_severity\",\n      \"type\": \"quantitative\",\n      \"title\": \"Average Severity\",\n      \"scale\": {\n        \"scheme\": \"viridis\"\n      }\n    },\n    \"tooltip\": [\n      {\"field\": \"neighborhd\", \"type\": \"nominal\", \"title\": \"Neighborhood\"},\n      {\"field\": \"count\", \"type\": \"quantitative\", \"title\": \"Count\"},\n      {\"field\": \"avg_severity\", \"type\": \"quantitative\", \"title\": \"Avg Severity\", \"format\": \".2f\"},\n      {\"field\": \"avg_agreement\", \"type\": \"quantitative\", \"title\": \"Avg Agreement\", \"format\": \".2f\"}\n    ]\n  }\n}\n</code></pre> </p> <p>This bubble chart visualization is valuable because it: - Shows the distribution of issues across neighborhoods - Uses size to indicate the number of issues (larger bubbles = more issues) - Uses color to represent severity (darker colors = higher severity) - Enables comparison of neighborhoods for prioritization</p>"},{"location":"examples/detailed_examples/04-accessibility-analysis/#final-result","title":"Final result","text":"<p>This workflow creates a visual analytics system for sidewalk accessibility data. By combining spatial visualization with statistical analysis, urban planners and accessibility advocates can:</p> <ol> <li>Identify patterns in accessibility issues across the city</li> <li>Compare the distribution and severity of different types of issues</li> <li>Prioritize neighborhoods for accessibility improvements</li> <li>Make data-driven decisions for urban planning</li> </ol> <p>The interactive nature of the visualizations allows for exploration and discovery, giving insights into urban accessibility challenges. This can ultimately help make cities more accessible for everyone, particularly people with mobility impairments. </p>"},{"location":"examples/detailed_examples/05-flooding-complaints/","title":"Example: Flooding complaints By Zip code","text":"<p>Author: Shaun Shannon</p> <p>In this example, we will explore how Curio can perform simple operations to compute the total number of flooding complaints ordered by Zip Code using Pandas. Here is the overview of the dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/05-flooding-complaints/#step-1-load-in-the-flooding-complaints-dataset","title":"Step 1: Load in the flooding complaints dataset","text":"<p>This example uses the Flooding Complaints to 311 dataset from the Chicago Data Portal. To get started with performing data analysis on the dataset, it needs to be imported into Curio. The best way to do that is to use a Data Loading Node to do so. The following code will complete this step:</p> <pre><code>import pandas as pd\n\nsensor = pd.read_csv('Flooding_Complaints_to_311_20250402../data/.csv')\n\nreturn sensor\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/05-flooding-complaints/#step-2-performing-computational-analysis-to-find-complaints-per-zip-code","title":"Step 2: Performing computational analysis to find complaints per Zip code","text":"<p>Now that we loaded the dataset using the Data Loading Node, we can perform simple computational analyses using Curio's Computational Analysis Node to find the total number of complaints per zip code. Here is the code to do so:</p> <pre><code>def complaints_by_zip(df):\n\ngrouped = df[\"ZIP_CODE\"].fillna(\"UNKNOWN\").value_counts().reset_index()\n\ngrouped.columns = [\"ZIP_CODE\",  \"Complaint_Count\"]\n\nreturn grouped\n\nreturn complaints_by_zip(arg)\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/05-flooding-complaints/#step-3-display-results-using-a-table","title":"Step 3: Display results using a table","text":"<p>In the last step, we performed computational analysis to get the total number of complaints per zip code. A possible way to see these types of results is using a table. We will connect the Computational Analysis Node with a Table Node to display our results in Table format. You do not need to include any code in the Table Node.</p> <p></p>"},{"location":"examples/detailed_examples/06-interaction/","title":"Example: Adding interaction between Vega-Lite and UTK","text":"<p>This examples covers a simple interaction between a Vega-Lite plot and a UTK visualization. This examples uses the Project Sidewalk sample data available here.</p>"},{"location":"examples/detailed_examples/06-interaction/#step-1-loading-data","title":"Step 1: Loading data","text":"<p>Once you have downloaded the datasets, upload  them into Curio using the Upload Dataset functionality.</p>"},{"location":"examples/detailed_examples/06-interaction/#step-2-connecting-utk-to-vega-lite-plots","title":"Step 2: Connecting UTK to Vega-Lite plots","text":"<p>To link UTK interactions with Vega-Lite plots, create data and interaction edges from a data pool node like the image below:</p> <p></p> <p>Select \"Picking\" as the interaction for UTK. The nodes will be populated with pre-defined code. Linked interactions can only work if they are connected to a data pool as shown in the image.</p> <p>You can download a JSON specification with this example here.</p>"},{"location":"examples/detailed_examples/07-speed-camera/","title":"Example: Visual analytics of speed camera violations","text":"<p>Author: Ameer Mustafa, Filip Petrev, Sania Sohail, Aakash Kolli</p> <p>In this example, we will explore how Curio can facilitate the temporal analysis of urban mobility data by processing and aggregating tabular records to analyze and visualize trends in speed camera violations across Chicago.</p> <p>Here is the overview of the entire dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/07-speed-camera/#step-1-load-speed-camera-violatiions-data","title":"Step 1: Load speed camera violatiions data","text":"<p>We begin creating a Data Loading node to load the speed camera violations dataset into Curio.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Speed_Camera_Violations../data/../data/.csv\")\ndf.dropna(inplace=True)\nreturn df\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/07-speed-camera/#step-2-data-pool","title":"Step 2: Data Pool","text":"<p>Next, we create a Data Pool node, which passes the cleaned DataFrame to downstream nodes for further transformation and visualization.</p>"},{"location":"examples/detailed_examples/07-speed-camera/#step-3-data-transformation-top-5-cameras-by-violations-per-year","title":"Step 3: Data Transformation \u2013 Top 5 Cameras by Violations per Year","text":"<p>Now, we will create a Data transformation node connected to the output of Step 2:</p> <pre><code>import pandas as pd\n\ndf = arg\n\ndf['VIOLATION DATE'] = pd.to_datetime(df['VIOLATION DATE'], format='%m/%d/%Y')\n\ndf['Year'] = df['VIOLATION DATE'].dt.year\n\nyr_sum = (df.groupby(['CAMERA ID', 'Year'])['VIOLATIONS']\n          .sum()\n          .reset_index()\n          .rename(columns={'VIOLATIONS': 'avg_violations'}))\n\ntop_ids = (df.groupby('CAMERA ID')['VIOLATIONS']\n             .sum()\n             .sort_values(ascending=False)\n             .head(5)\n             .index\n             .tolist())\n\nyr_sum = yr_sum[yr_sum['CAMERA ID'].isin(top_ids)]\n\ncamera_pos = (df.groupby('CAMERA ID')[['LATITUDE', 'LONGITUDE']]\n                .mean()\n                .reset_index())\n\nyr_sum = yr_sum.merge(camera_pos, on='CAMERA ID')\n\nreturn yr_sum\n</code></pre> <p></p> <p>This analysis aggregates the violations by camera and year, identifying the top 5 cameras with the highest total violations.</p>"},{"location":"examples/detailed_examples/07-speed-camera/#step-4-linked-view-visualization-interactive-exploration","title":"Step 4: Linked View Visualization \u2013 Interactive Exploration","text":"<p>We then create a linked view visualization using the 2D Plot (Vega-Lite) node. This visualization includes both a stacked bar chart and a line chart to explore total violations over time.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"table\" },\n  \"config\": { \"bar\": { \"continuousBandSize\": 18 } },\n  \"hconcat\": [\n    {\n      \"width\": 320,\n      \"height\": 260,\n      \"selection\": { \"yrBrush\": { \"type\": \"interval\", \"encodings\": [\"x\"] } },\n      \"mark\": { \"type\": \"bar\" },\n      \"encoding\": {\n        \"x\": { \"field\": \"Year\", \"type\": \"quantitative\", \"title\": \"Year\" },\n        \"y\": {\n          \"aggregate\": \"sum\",\n          \"field\": \"avg_violations\",\n          \"type\": \"quantitative\",\n          \"title\": \"Total Violations\"\n        },\n        \"color\": {\n          \"field\": \"CAMERA ID\",\n          \"type\": \"nominal\",\n          \"legend\": { \"title\": \"Camera ID\" }\n        }\n      }\n    },\n    {\n      \"width\": 320,\n      \"height\": 260,\n      \"transform\": [\n        { \"filter\": { \"selection\": \"yrBrush\" } },\n        {\n          \"aggregate\": [\n            { \"op\": \"sum\", \"field\": \"avg_violations\", \"as\": \"total\" }\n          ],\n          \"groupby\": [\"Year\"]\n        },\n        { \"sort\": { \"field\": \"Year\" } }\n      ],\n      \"mark\": { \"type\": \"line\", \"point\": true },\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"Year\",\n          \"type\": \"quantitative\",\n          \"title\": \"Year (brush range)\"\n        },\n        \"y\": {\n          \"field\": \"total\",\n          \"type\": \"quantitative\",\n          \"title\": \"Total Violations\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/07-speed-camera/#final-result","title":"Final result","text":"<p>This example demonstrates how Curio can be used for a detailed temporal analysis of urban safety data. By transforming and aggregating violation records, we can generate interactive visualizations like stacked bar charts and linked views to effectively identify and compare trends over time. This workflow allows for a deeper understanding of violation patterns and the performance of specific camera locations.</p>"},{"location":"examples/detailed_examples/08-red-light-violation/","title":"Example: Urban traffic violation analysis","text":"<p>Authors: Niketan Doddamani, Purvi Vadher, Sanjay Kalaivanan, Aakash Kolli</p> <p>In this example, we will explore how Curio can be used to construct a comprehensive dataflow for analyzing and visualizing urban red-light traffic violations. This example demonstrates how to load, process, and visualize urban traffic data, enabling insights into violation patterns, enforcement effectiveness, and spatial trends.</p> <p>Here is the overview of the entire dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/08-red-light-violation/#step-1-load-the-red-light-violation-data","title":"Step 1: Load the red-light violation data","text":"<p>We begin by loading the red-light violation data into Curio using a Data Loading node. This step reads the CSV file and prepares it for further processing.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Filtered_Data../data/../data/.csv\")\nreturn df\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/08-red-light-violation/#step-2-dataflow-to-observe-seasonal-violation-trends","title":"Step 2: Dataflow to Observe Seasonal Violation Trends","text":"<p>We will create a basic dataflow that consists of the Data Loading node, a Data Cleaning node, and a 2D Plot (Vega-Lite) node. This will allow us to observe seasonal trends in red-light violations over time.</p> <p></p> <p>We create a Data Cleaning node and connect it to the Data Loading node. This node will be used to extract relevant temporal features from the data and prepare it for trend analysis. This involves parsing dates, extracting months and years, and assigning seasons.</p> <pre><code>import pandas as pd\n\ndf = arg.copy()\ndf['VIOLATION DATE'] = pd.to_datetime(df['VIOLATION DATE'])\ndf['Year'] = df['VIOLATION DATE'].dt.year\ndf['Month'] = df['VIOLATION DATE'].dt.month\n\ndef assign_season(month):\n    if month in [12, 1, 2]:\n        return \"Winter\"\n    elif month in [3, 4, 5]:\n        return \"Spring\"\n    elif month in [6, 7, 8]:\n        return \"Summer\"\n    else:\n        return \"Fall\"\n\ndf['Season'] = df['Month'].apply(assign_season)\n\ndf_trend = df.groupby(['VIOLATION DATE', 'Year', 'Season'])['VIOLATIONS'].sum().reset_index()\ndf_trend['VIOLATION DATE'] = df_trend['VIOLATION DATE'].astype(str)\n\nreturn pd.DataFrame(df_trend)\n</code></pre> <p>Now, to visualize the seasonal trends in red-light violations, we then create a 2D Plot (Vega-Lite) node that shows the number of violations over time, categorized by season.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"width\": 750,\n  \"height\": 400,\n  \"title\": \"Seasonal Violation Trend (Daily)\",\n  \"mark\": {\n    \"type\": \"line\",\n    \"point\": true\n  },\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"VIOLATION DATE\",\n      \"type\": \"temporal\",\n      \"title\": \"Date\",\n      \"axis\": {\n        \"format\": \"%Y %b\",\n        \"labelAngle\": -45\n      }\n    },\n    \"y\": {\n      \"field\": \"VIOLATIONS\",\n      \"type\": \"quantitative\",\n      \"title\": \"Violations\"\n    },\n    \"color\": {\n      \"field\": \"Season\",\n      \"type\": \"nominal\",\n      \"title\": \"Season\",\n      \"scale\": {\n        \"domain\": [\"Winter\", \"Spring\", \"Summer\", \"Fall\"],\n        \"range\": [\"#1f77b4\", \"#2ca02c\", \"#ff7f0e\", \"#9467bd\"]\n      }\n    },\n    \"tooltip\": [\n      { \"field\": \"VIOLATION DATE\", \"type\": \"temporal\", \"title\": \"Date\" },\n      { \"field\": \"Season\", \"type\": \"nominal\" },\n      { \"field\": \"VIOLATIONS\", \"type\": \"quantitative\" }\n    ]\n  }\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/08-red-light-violation/#step-3-dataflow-to-observe-monthly-and-seasonal-violation-trends","title":"Step 3: Dataflow to Observe Monthly and Seasonal Violation Trends","text":"The top node is the Data Loading node from Step 1. The bottom node is the Data Cleaning node created in this step. <p>Next, we create a new Data Cleaning node (bottom node in the above image) to prepare the data for a heatmap visualization. This node will aggregate violations by month and year, allowing us to analyze monthly trends.</p> <p>Then, we create a Merge Flow to combine both Data Cleaning nodes, allowing us to analyze both daily violation trends and monthly totals.</p> <pre><code>import pandas as pd\n\ndf = arg.copy()\ndf['VIOLATION DATE'] = pd.to_datetime(df['VIOLATION DATE'])\ndf['Year'] = df['VIOLATION DATE'].dt.year\ndf['Month'] = df['VIOLATION DATE'].dt.month\n\nheatmap_data = df.groupby(['Year', 'Month'])['VIOLATIONS'].sum().reset_index()\n\nreturn pd.DataFrame(heatmap_data)\n</code></pre> <p></p> <p>Then, we create a Computational Analysis node that merges the daily violation data with the monthly heatmap data.</p> <pre><code>import pandas as pd\n\ndf_trend = pd.DataFrame(arg[0])\nheatmap_data = pd.DataFrame(arg[1])\n\ndf_trend['VIOLATION DATE'] = pd.to_datetime(df_trend['VIOLATION DATE'])\n\nsummary = heatmap_data.groupby('Year')['VIOLATIONS'].sum().reset_index()\n\nmerged = df_trend.merge(summary, on='Year', how='left')\n\nfinal = merged[['VIOLATION DATE', 'VIOLATIONS_x', 'Season', 'VIOLATIONS_y']]\n\nfinal.columns = ['VIOLATION DATE', 'Daily Violations', 'Season', 'Yearly Total']\n\nfinal['VIOLATION DATE'] = final['VIOLATION DATE'].astype(str)\n\narray_data = final.to_dict(orient='records')\n\nshape = [final.shape[0], final.shape[1]]\n\nreturn pd.DataFrame(array_data)\n</code></pre> <p>We then create a Data Cleaning node to ensure the data is in the correct format for visualization. This includes converting date fields to strings and ensuring numeric fields are properly formatted.</p> <pre><code>import pandas as pd\n\ndf = arg.copy()\n\ndf['VIOLATION DATE'] = pd.to_datetime(df['VIOLATION DATE'], errors='coerce')\n\nif 'Year' not in df.columns:\n    df['Year'] = df['VIOLATION DATE'].dt.year\nif 'Month' not in df.columns:\n    df['Month'] = df['VIOLATION DATE'].dt.month\n\ndf['Daily Violations'] = pd.to_numeric(df['Daily Violations'], errors='coerce')\ndf['Yearly Total'] = pd.to_numeric(df['Yearly Total'], errors='coerce')\n\ndf['VIOLATION DATE'] = df['VIOLATION DATE'].astype(str)\n\nreturn pd.DataFrame(df)\n</code></pre> <p>Lastly, we create a 2D Plot (Vega-Lite) node to visualize both monthly and seasonal trends in red-light violations. This visualization will include a heatmap for monthly violations and a line chart for daily violations, allowing users to explore trends interactively.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"hconcat\": [\n    {\n      \"width\": 300,\n      \"height\": 300,\n      \"title\": \"Monthly Violations Heatmap\",\n      \"params\": [\n        {\n          \"name\": \"yearFilter\",\n          \"select\": {\n            \"type\": \"point\",\n            \"fields\": [\"Year\"],\n            \"on\": \"click\"\n          }\n        }\n      ],\n      \"mark\": \"rect\",\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"Month\",\n          \"type\": \"ordinal\",\n          \"title\": \"Month\"\n        },\n        \"y\": {\n          \"field\": \"Year\",\n          \"type\": \"ordinal\",\n          \"title\": \"Year\"\n        },\n        \"color\": {\n          \"aggregate\": \"sum\",\n          \"field\": \"Yearly Total\",\n          \"type\": \"quantitative\",\n          \"scale\": {\n            \"scheme\": \"orangered\"\n          },\n          \"title\": \"Violations\"\n        },\n        \"tooltip\": [\n          { \"field\": \"Year\", \"type\": \"ordinal\" },\n          { \"field\": \"Month\", \"type\": \"ordinal\" },\n          {\n            \"aggregate\": \"sum\",\n            \"field\": \"Yearly Total\",\n            \"type\": \"quantitative\",\n            \"title\": \"Total Violations\"\n          }\n        ]\n      }\n    },\n    {\n      \"width\": 600,\n      \"height\": 300,\n      \"title\": \"Seasonal Violation Trend (Daily)\",\n      \"transform\": [\n        {\n          \"filter\": \"yearFilter.Year == null || datum.Year == yearFilter.Year\"\n        }\n      ],\n      \"mark\": \"line\",\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"VIOLATION DATE\",\n          \"type\": \"temporal\",\n          \"title\": \"Date\"\n        },\n        \"y\": {\n          \"field\": \"Daily Violations\",\n          \"type\": \"quantitative\",\n          \"title\": \"Violations\"\n        },\n        \"color\": {\n          \"field\": \"Season\",\n          \"type\": \"nominal\"\n        },\n        \"tooltip\": [\n          { \"field\": \"VIOLATION DATE\", \"type\": \"temporal\" },\n          { \"field\": \"Daily Violations\", \"type\": \"quantitative\" },\n          { \"field\": \"Season\", \"type\": \"nominal\" }\n        ]\n      }\n    }\n  ]\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/08-red-light-violation/#step-4-dataflow-to-create-stacked-area-chart","title":"Step 4: Dataflow to Create Stacked Area Chart","text":"<p>First, we create a Data Cleaning node to prepare the data for seasonal analysis. This node will extract the month and year from the violation date, assign seasons, and aggregate violations by year and season.</p> <pre><code>import pandas as pd\n\ndf = arg\n\ndf[\"VIOLATION DATE\"] = pd.to_datetime(df[\"VIOLATION DATE\"])\n\ndf[\"Month\"] = df[\"VIOLATION DATE\"].dt.month\ndf[\"Year\"] = df[\"VIOLATION DATE\"].dt.year\n\ndef assign_season(month):\n    if month in [12, 1, 2]:\n        return \"Winter\"\n    elif month in [3, 4, 5]:\n        return \"Spring\"\n    elif month in [6, 7, 8]:\n        return \"Summer\"\n    else:\n        return \"Fall\"\n\ndf[\"Season\"] = df[\"Month\"].apply(assign_season)\n\ndf_seasonal = df.groupby([\"Year\", \"Season\"])[\"VIOLATIONS\"].sum().reset_index()\n\nreturn pd.DataFrame(df_seasonal)\n</code></pre> <p>Then, we use a Vega-Lite node to create a stacked area chart to visualize how red-light violations change over time across different seasons.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"title\": \"Seasonal Red-Light Violations Over Time\",\n  \"mark\": \"area\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"Year\",\n      \"type\": \"ordinal\",\n      \"title\": \"Year\"\n    },\n    \"y\": {\n      \"field\": \"VIOLATIONS\",\n      \"type\": \"quantitative\",\n      \"title\": \"Total Violations\"\n    },\n    \"color\": {\n      \"field\": \"Season\",\n      \"type\": \"nominal\",\n      \"title\": \"Season\"\n    },\n    \"tooltip\": [\n      { \"field\": \"Year\", \"type\": \"ordinal\" },\n      { \"field\": \"Season\", \"type\": \"nominal\" },\n      { \"field\": \"VIOLATIONS\", \"type\": \"quantitative\" }\n    ]\n  },\n  \"width\": 600,\n  \"height\": 400\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/08-red-light-violation/#step-5-data-flow-to-visualize-top-intersections-by-violations","title":"Step 5: Data Flow to Visualize Top Intersections by Violations","text":"<p>First, we create a Data Cleaning node to prepare the data for visualizing the top intersections with the most violations. This involves aggregating violations by intersection and year, and ranking them to identify the top 3 intersections for each year.</p> <p></p> <pre><code>import pandas as pd\n\ndf = arg\ndf[\"VIOLATION DATE\"] = pd.to_datetime(df[\"VIOLATION DATE\"])\ndf[\"Year\"] = df[\"VIOLATION DATE\"].dt.year\n\ngrouped = df.groupby([\"INTERSECTION\", \"Year\"])[\"VIOLATIONS\"].sum().reset_index()\ngrouped[\"Rank\"] = grouped.groupby(\"Year\")[\"VIOLATIONS\"].rank(ascending=False, method=\"first\")\n\ntop3 = grouped[grouped[\"Rank\"] &lt;= 3]\n\nreturn pd.DataFrame(top3)\n</code></pre> <p>We then make a 2D Plot (Vega-Lite) node to visualize the top intersections using a stacked bar chart.</p> <p><pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"title\": \"Top 3 Intersections with Most Violations by Year\",\n  \"mark\": \"bar\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"Year\",\n      \"type\": \"ordinal\",\n      \"title\": \"Year\"\n    },\n    \"y\": {\n      \"field\": \"VIOLATIONS\",\n      \"type\": \"quantitative\",\n      \"title\": \"Violations\"\n    },\n    \"color\": {\n      \"field\": \"INTERSECTION\",\n      \"type\": \"nominal\",\n      \"title\": \"Intersection\"\n    },\n    \"tooltip\": [\n      { \"field\": \"INTERSECTION\", \"type\": \"nominal\" },\n      { \"field\": \"VIOLATIONS\", \"type\": \"quantitative\" }\n    ]\n  },\n  \"width\": 600,\n  \"height\": 400\n}\n</code></pre> </p>"},{"location":"examples/detailed_examples/08-red-light-violation/#step-6-dataflow-to-visualize-camera-count-vs-violation-distribution","title":"Step 6: Dataflow to Visualize Camera Count vs. Violation Distribution","text":"<p>First, we create two Data Cleaning nodes to prepare the data for further analysis. The first node aggregates camera counts and violations by intersection, while the second node calculates the percentage reduction in violations over time.</p> <p>We then create a Merge Flow to combine the results from both Data Cleaning nodes, allowing us to analyze camera deployment and compliance trends.</p>"},{"location":"examples/detailed_examples/08-red-light-violation/#first-data-cleaning-node","title":"First Data Cleaning Node","text":"<pre><code>import pandas as pd\nimport numpy as np\n\ndf = arg.copy()\ndf[\"VIOLATION DATE\"] = pd.to_datetime(df[\"VIOLATION DATE\"])\ndf[\"Year\"] = df[\"VIOLATION DATE\"].dt.year\n\n\ngrouped = df.groupby(\"INTERSECTION\").agg({\n    \"CAMERA ID\": \"nunique\",\n    \"VIOLATIONS\": \"sum\"\n}).reset_index().rename(columns={\"CAMERA ID\": \"CAMERA_COUNT\"})\n\ngrouped[\"CAMERA_BIN\"] = grouped[\"CAMERA_COUNT\"].apply(lambda x: \"4+\" if x &gt;= 4 else str(x))\n\n\ncamera_order = {\"1\": 1, \"2\": 2, \"3\": 3, \"4+\": 4}\ngrouped[\"x_base\"] = grouped[\"CAMERA_BIN\"].map(camera_order)\nnp.random.seed(42)\ngrouped[\"jittered_x\"] = grouped[\"x_base\"] + np.random.uniform(-0.2, 0.2, size=len(grouped))\n\n\nreturn pd.DataFrame(grouped)\n</code></pre>"},{"location":"examples/detailed_examples/08-red-light-violation/#second-data-cleaning-node","title":"Second Data Cleaning Node","text":"<pre><code>import pandas as pd\n\ndf = arg.copy()\n\ndf[\"VIOLATION DATE\"] = pd.to_datetime(df[\"VIOLATION DATE\"])\ndf[\"Year\"] = df[\"VIOLATION DATE\"].dt.year\n\ntrend = df.groupby(['INTERSECTION', 'Year'])['VIOLATIONS'].sum().reset_index()\nfirst = trend.groupby('INTERSECTION').first().reset_index()\nlast = trend.groupby('INTERSECTION').last().reset_index()\n\nchange = first.merge(last, on='INTERSECTION', suffixes=('_first', '_last'))\nchange = change[change['VIOLATIONS_first'] &gt; 0]\n\nchange['Percent_Reduction'] = ((change['VIOLATIONS_first'] - change['VIOLATIONS_last']) / change['VIOLATIONS_first']) * 100\n\n\nreturn pd.DataFrame(change)\n</code></pre> <p>We then create a Merge Flow to combine the results from both Data Cleaning nodes, allowing us to analyze camera deployment and compliance trends.</p> <p></p> <p>Now, we create a Computational Analysis node to merge the results from the two Data Cleaning nodes. This node will combine the aggregated camera counts and violations with the percentage reduction in violations, preparing the data for visualization.</p> <pre><code>import pandas as pd\n\ngrouped = pd.DataFrame(arg[0])\nchange = pd.DataFrame(arg[1])\n\nmerged = grouped.merge(\n    change[['INTERSECTION', 'Percent_Reduction']],\n    on='INTERSECTION',\n    how='left'\n)\n\nmerged['VIOLATIONS'] = pd.to_numeric(merged['VIOLATIONS'], errors='coerce')\nmerged['Percent_Reduction'] = pd.to_numeric(merged['Percent_Reduction'], errors='coerce')\n\nmerged = merged.dropna(subset=['Percent_Reduction'])\n\narray_data = merged.to_dict(orient='records')\n\nshape = [merged.shape[0], merged.shape[1]]\n\nreturn pd.DataFrame(array_data)\n</code></pre> <p>Next, we create a Data Cleaning node to ensure the data is in the correct format for visualization. This includes converting numeric fields to appropriate types and dropping any rows with missing values.</p> <pre><code>import pandas as pd\n\ndf = arg.copy()\n\ndf['VIOLATIONS'] = pd.to_numeric(df['VIOLATIONS'], errors='coerce')\ndf['CAMERA_COUNT'] = pd.to_numeric(df['CAMERA_COUNT'], errors='coerce')\ndf['Percent_Reduction'] = pd.to_numeric(df['Percent_Reduction'], errors='coerce')\ndf['jittered_x'] = pd.to_numeric(df['jittered_x'], errors='coerce')\n\ndf = df.dropna(subset=['VIOLATIONS', 'CAMERA_COUNT', 'Percent_Reduction', 'jittered_x'])\n\n\nreturn pd.DataFrame(df)\n</code></pre> <p>Lastly, we use a linked visualization to show violation distribution and compliance by camera count.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"params\": [\n    {\n      \"name\": \"cameraFilter\",\n      \"bind\": {\n        \"input\": \"select\",\n        \"options\": [\"1\", \"2\", \"3\", \"4+\"],\n        \"labels\": [\"1 Camera\", \"2 Cameras\", \"3 Cameras\", \"4+ Cameras\"]\n      }\n    }\n  ],\n  \"hconcat\": [\n    {\n      \"width\": 500,\n      \"mark\": \"boxplot\",\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"CAMERA_BIN\",\n          \"type\": \"nominal\",\n          \"title\": \"Camera Count\"\n        },\n        \"y\": {\n          \"field\": \"VIOLATIONS\",\n          \"type\": \"quantitative\",\n          \"title\": \"Violations\"\n        },\n        \"color\": {\n          \"field\": \"CAMERA_BIN\",\n          \"type\": \"nominal\"\n        }\n      }\n    },\n    {\n      \"width\": 500,\n      \"mark\": {\n        \"type\": \"bar\",\n        \"cursor\": \"pointer\"\n      },\n      \"transform\": [\n        { \"filter\": \"cameraFilter == null || datum.CAMERA_BIN == cameraFilter\" }\n      ],\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"Percent_Reduction\",\n          \"type\": \"quantitative\",\n          \"title\": \"Percent Reduction\"\n        },\n        \"y\": {\n          \"field\": \"INTERSECTION\",\n          \"type\": \"nominal\",\n          \"sort\": \"-x\",\n          \"title\": \"Intersection\"\n        },\n        \"color\": {\n          \"field\": \"Percent_Reduction\",\n          \"type\": \"quantitative\",\n          \"scale\": { \"scheme\": \"blues\" }\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/08-red-light-violation/#step-7-dataflow-to-visualize-spatial-distribution-of-red-light-violations","title":"Step 7: Dataflow to Visualize Spatial Distribution of Red-Light Violations","text":"<p>We create two Data Cleaning nodes to prepare the data for spatial visualization. The first node extracts the year from the violation date and formats it, while the second node aggregates violations by intersection, latitude, and longitude, preparing it for mapping.</p> <p>We then create a Merge Flow to combine the results from both Data Cleaning nodes.</p>"},{"location":"examples/detailed_examples/08-red-light-violation/#first-data-cleaning-node_1","title":"First Data Cleaning Node","text":"<pre><code>import pandas as pd\n\ndf =  pd.DataFrame(arg)\n\ndf['VIOLATION DATE'] = pd.to_datetime(df['VIOLATION DATE'], errors='coerce')\n\ndf['Year'] = df['VIOLATION DATE'].dt.year\n\ndf['VIOLATION DATE'] = df['VIOLATION DATE'].astype(str)\n\n\nreturn pd.DataFrame(df)\n</code></pre>"},{"location":"examples/detailed_examples/08-red-light-violation/#second-data-cleaning-node_1","title":"Second Data Cleaning Node","text":"<pre><code>import pandas as pd\n\ndf = arg\n\ndf_map = df.groupby(['INTERSECTION', 'LATITUDE', 'LONGITUDE']).agg({\n    'VIOLATIONS': 'sum',\n    'CAMERA ID': 'nunique'\n}).reset_index().rename(columns={\n    'VIOLATIONS': 'TOTAL_VIOLATIONS',\n    'CAMERA ID': 'CAMERA_COUNT'\n})\n\n\ndf_map = df_map.dropna(subset=['LATITUDE', 'LONGITUDE'])\n\ndf_map['CAMERA_BIN'] = df_map['CAMERA_COUNT'].apply(lambda x: \"4+\" if x &gt;= 4 else str(int(x)))\n\nreturn pd.DataFrame(df_map)\n</code></pre> <p>Next, we create a Computational Analysis node to prepare the data for spatial visualization. This node aggregates violations by intersection, latitude, and longitude, and calculates the total number of violations and unique camera counts.</p> <pre><code>import pandas as pd\n\ndf_base = pd.DataFrame(arg[0])\ndf_additional = pd.DataFrame(arg[1])\n\ndf_map = df_base.groupby(['INTERSECTION', 'LATITUDE', 'LONGITUDE']).agg({\n    'VIOLATIONS': 'sum',\n    'CAMERA ID': 'nunique'\n}).reset_index().rename(columns={\n    'VIOLATIONS': 'TOTAL_VIOLATIONS',\n    'CAMERA ID': 'CAMERA_COUNT'\n})\n\ndf_map = df_map.dropna(subset=['LATITUDE', 'LONGITUDE'])\n\ndf_map[\"CAMERA_BIN\"] = df_map[\"CAMERA_COUNT\"].apply(lambda x: \"4+\" if x &gt;= 4 else str(int(x)))\n\narray_data = df_map.to_dict(orient='records')\n\nreturn pd.DataFrame(array_data)\n</code></pre> <p>We then create a Data Cleaning node to ensure the data is in the correct format for spatial visualization. This includes converting numeric fields to appropriate types and ensuring latitude and longitude are numeric.</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame(arg)\n\ndf['TOTAL_VIOLATIONS'] = pd.to_numeric(df['TOTAL_VIOLATIONS'], errors='coerce')\ndf['CAMERA_COUNT'] = pd.to_numeric(df['CAMERA_COUNT'], errors='coerce')\n\ndf['CAMERA_BIN'] = df['CAMERA_BIN'].astype(str)\n\ndf['LATITUDE'] = pd.to_numeric(df['LATITUDE'], errors='coerce')\ndf['LONGITUDE'] = pd.to_numeric(df['LONGITUDE'], errors='coerce')\n\nreturn pd.DataFrame(df)\n</code></pre> <p>Lastly, we create a 2D Plot (Vega-Lite) node to visualize the spatial distribution of red-light violations across intersections in Chicago. This visualization will allow users to explore the geographic patterns of violations and camera deployment.</p> <p></p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"hconcat\": [\n    {\n      \"width\": 600,\n      \"height\": 500,\n      \"title\": \"Spatial Map \u2013 Select Area to Filter\",\n      \"params\": [\n        {\n          \"name\": \"spatialBrush\",\n          \"select\": {\n            \"type\": \"interval\",\n            \"encodings\": [\"x\", \"y\"]\n          }\n        }\n      ],\n      \"mark\": \"circle\",\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"LONGITUDE\",\n          \"type\": \"quantitative\",\n          \"scale\": { \"domain\": [-87.95, -87.5] },\n          \"title\": \"Longitude\"\n        },\n        \"y\": {\n          \"field\": \"LATITUDE\",\n          \"type\": \"quantitative\",\n          \"scale\": { \"domain\": [41.6, 42.1] },\n          \"title\": \"Latitude\"\n        },\n        \"size\": {\n          \"field\": \"TOTAL_VIOLATIONS\",\n          \"type\": \"quantitative\",\n          \"scale\": { \"range\": [20, 800] },\n          \"title\": \"Total Violations\"\n        },\n        \"color\": {\n          \"field\": \"CAMERA_BIN\",\n          \"type\": \"nominal\",\n          \"title\": \"Camera Count\",\n          \"scale\": { \"scheme\": \"plasma\" }\n        },\n        \"tooltip\": [\n          { \"field\": \"INTERSECTION\", \"type\": \"nominal\" },\n          { \"field\": \"TOTAL_VIOLATIONS\", \"type\": \"quantitative\" },\n          { \"field\": \"CAMERA_COUNT\", \"type\": \"quantitative\" }\n        ]\n      }\n    },\n    {\n      \"width\": 400,\n      \"height\": 500,\n      \"title\": \"Top Intersections by Total Violations (Filtered by Spatial Selection)\",\n      \"mark\": \"bar\",\n      \"transform\": [\n        {\n          \"filter\": { \"param\": \"spatialBrush\" }\n        },\n        {\n          \"window\": [{ \"op\": \"rank\", \"as\": \"rank\" }],\n          \"sort\": [{ \"field\": \"TOTAL_VIOLATIONS\", \"order\": \"descending\" }]\n        },\n        {\n          \"filter\": \"datum.rank &lt;= 15\"\n        }\n      ],\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"TOTAL_VIOLATIONS\",\n          \"type\": \"quantitative\",\n          \"title\": \"Total Violations\"\n        },\n        \"y\": {\n          \"field\": \"INTERSECTION\",\n          \"type\": \"nominal\",\n          \"sort\": \"-x\",\n          \"title\": \"Intersection\"\n        },\n        \"color\": {\n          \"field\": \"TOTAL_VIOLATIONS\",\n          \"type\": \"quantitative\",\n          \"scale\": { \"scheme\": \"blues\" },\n          \"legend\": null\n        },\n        \"tooltip\": [\n          { \"field\": \"INTERSECTION\", \"type\": \"nominal\" },\n          { \"field\": \"TOTAL_VIOLATIONS\", \"type\": \"quantitative\" }\n        ]\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/detailed_examples/08-red-light-violation/#final-result","title":"Final result","text":"<p>This tutorial demonstrated how Curio can be used to build a complete urban data analysis workflow, from data loading and cleaning to advanced interactive and spatial visualizations. By leveraging Curio\u2019s modular dataflow and visualization capabilities, we can uncover temporal, spatial, and compliance patterns in urban traffic violation data, supporting informed policy and enforcement decisions.</p>"},{"location":"examples/detailed_examples/09-energy-efficiency/","title":"Example: Visual analytics of building energy efficiency","text":"<p>Authors: Himanshu Dongre, Aakash Kolli</p> <p>In this example, we will learn how Curio can be used to perform a comparative analysis of building energy performance. The process involves loading and cleaning a dataset of Chicago buildings, then making a visualization to benchmark the energy efficiency across different property types.</p> <p>Here is the overview of the entire dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/09-energy-efficiency/#step-1-load-energy-efficieency-data","title":"Step 1: Load energy efficieency data","text":"<p>We begin by loading the energy efficiency dataset into Curio using a Data Loading node.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"energy_dataset../data/../data/.csv\")\nreturn df\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/09-energy-efficiency/#step-2-data-cleaning-and-processing","title":"Step 2: Data cleaning and processing","text":"<p>Next, we create a Data Cleaning node to preprocess the data to retain only the key attributes and remove incomplete rows. We also convert the ZIP code to an integer for consistency.</p> <pre><code>iedf  = arg[['Data Year', 'ID', 'Property Name', 'Address', 'ZIP Code', 'Chicago Energy Rating', 'Community Area', 'Primary Property Type', 'Gross Floor Area - Buildings (sq ft)', 'Year Built', '# of Buildings', 'ENERGY STAR Score', 'Site EUI (kBtu/sq ft)', 'Source EUI (kBtu/sq ft)', 'Weather Normalized Site EUI (kBtu/sq ft)', 'Weather Normalized Source EUI (kBtu/sq ft)', 'Total GHG Emissions (Metric Tons CO2e)', 'GHG Intensity (kg CO2e/sq ft)', 'Latitude', 'Longitude', 'Location']]\n\n# Rename the data columns for consistency and easy use\nedf.columns = ['Year', 'ID', 'Property Name', 'Address', 'ZIP Code', 'Chicago Energy Rating', 'Community Area', 'Primary Property Type', 'Gross Floor Area', 'Year Built', '# of Buildings', 'ENERGY STAR Score', 'Site EUI', 'Source EUI', 'Weather Normalized Site EUI', 'Weather Normalized Source EUI', 'Total GHG Emissions', 'GHG Intensity', 'Latitude', 'Longitude', 'Location']\n\n# Filter out rows with missing data\nedf = edf.dropna()\nedf['ZIP Code'] = edf['ZIP Code'].astype(int)\n\nreturn edf\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/09-energy-efficiency/#step-3-visualization-mean-and-median-chart","title":"Step 3: Visualization \u2013 Mean and Median Chart","text":"<p>Then, we create a 2D Plot (Vega-Lite) node to create a bar and tick chart, which compares energy efficiency metrics across property types. This chart displays the mean and median of a selected metric (e.g., weather normalized site EUI) for each property type.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"table\" },\n  \"title\": \"Mean and Median Weather Normalized Site EUI by Property Type\",\n  \"width\": 700,\n  \"height\": 400,\n  \"layer\": [\n    {\n      \"mark\": \"bar\",\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"PRIMARY PROPERTY TYPE\",\n          \"type\": \"nominal\",\n          \"sort\": \"-y\",\n          \"title\": \"Property Type\"\n        },\n        \"y\": {\n          \"aggregate\": \"mean\",\n          \"field\": \"WEATHER NORMALIZED SITE EUI (KBTU/SQ FT)\",\n          \"type\": \"quantitative\",\n          \"title\": \"Mean Weather Normalized Site EUI\"\n        },\n        \"color\": {\n          \"field\": \"PRIMARY PROPERTY TYPE\",\n          \"type\": \"nominal\",\n          \"legend\": null\n        }\n      }\n    },\n    {\n      \"mark\": { \"type\": \"tick\", \"color\": \"black\", \"size\": 40 },\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"PRIMARY PROPERTY TYPE\",\n          \"type\": \"nominal\"\n        },\n        \"y\": {\n          \"aggregate\": \"median\",\n          \"field\": \"WEATHER NORMALIZED SITE EUI (KBTU/SQ FT)\",\n          \"type\": \"quantitative\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/09-energy-efficiency/#final-result","title":"Final result","text":"<p>The final result of this workflow is a layered chart that presents both the mean and median energy use intensity for various building types. Displaying both metrics reveals important details about the data's distribution as a large gap between the mean and median values for a category shows that a few highly inefficient buildings are skewing the average. Recognizing this distinction helps in developing more effective energy efficiency policies and programs.</p>"},{"location":"examples/detailed_examples/10-green-roofs/","title":"Example: Visual analytics of green roofs","text":"<p>Authors: Samuel Haddad, Lorena Castillejo, Aakash Kolli</p> <p>In this example, we will explore how Curio can facilitate the spatial analysis of urban green infrastructure by integrating tabular and geospatial data to analyze and visualize the distribution and density of green roofs in Chicago.</p> <p></p> <p>Here is the overview of the entire dataflow pipeline:</p> <p></p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/10-green-roofs/#step-1-load-the-green-roofs-data","title":"Step 1: Load the green roofs data","text":"<p>We begin the first dataflow by loading the green roofs dataset into Curio using a Data Loading node. This step reads the CSV file and prepares it for further processing.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Green_Roofs../data/../data/.csv\")\nreturn df\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/10-green-roofs/#step-2-data-cleaning-and-processing","title":"Step 2: Data cleaning and processing","text":"<p>Next, we clean the dataset by creating a Data Cleaning node and connecting it to the previous node.</p> <pre><code>import pandas as pd\n\ndf = arg\ndf.fillna(0, inplace=True)\n\nreturn df\n</code></pre>"},{"location":"examples/detailed_examples/10-green-roofs/#step-3-histogram-distribution-of-green-roof-sizes","title":"Step 3: Histogram \u2013 Distribution of Green Roof Sizes","text":"<p>Now, we use a 2D Plot (Vega-Lite) node to create a histogram visualization to understand the overall distribution of green roof sizes.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"description\": \"Histogram of Total Roof Size of Buildings in Chicago (log-scaled)\",\n  \"data\": {\n    \"name\": \"data\"\n  },\n  \"transform\": [\n    { \"filter\": \"datum.TOTAL_ROOF_SQFT &gt; 0\" },\n    {\n      \"calculate\": \"log(datum.TOTAL_ROOF_SQFT) / log(10)\",\n      \"as\": \"log_roof_size\"\n    }\n  ],\n  \"mark\": \"bar\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"log_roof_size\",\n      \"bin\": { \"maxbins\": 30 },\n      \"axis\": {\n        \"title\": \"Total Roof Size (sqft)\",\n        \"values\": [3, 4, 5, 6],\n        \"labelExpr\": \"'10^' + datum.value\"\n      }\n    },\n    \"y\": {\n      \"aggregate\": \"count\",\n      \"type\": \"quantitative\",\n      \"axis\": {\n        \"title\": \"Number of Buildings\"\n      }\n    }\n  }\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/10-green-roofs/#step-4-data-loading","title":"Step 4: Data Loading","text":"<p>For the next dataflow, we start with creating a Data Loading node to load in both datasets and join them.</p> <pre><code>import geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import Point\n# Read the green roofs dataset\ngreen_roofs_df = pd.read_csv('Green_Roofs../data/.csv')\n\n# Create the dataset into geo dataframe using latitude and longitude columns\ngeometry = [Point(xy) for xy in zip(green_roofs_df['LONGITUDE'], green_roofs_df['LATITUDE'])]\ngreen_roofs_df = gpd.GeoDataFrame(green_roofs_df, geometry=geometry, crs=4326)\nchicago = gpd.read_file(\"chicago.geojson\")\n\n# Joining the green roofs dataset with the chicago neighborhood geojson file\njoined = gpd.sjoin(green_roofs_df, chicago, predicate='within')\nreturn joined\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/10-green-roofs/#step-5-data-pool","title":"Step 5: Data Pool","text":"<p>Next, we create a Data Pool node, which passes the joined dataset to downstream nodes for further transformation and visualization.</p>"},{"location":"examples/detailed_examples/10-green-roofs/#step-6-spatial-distribution-of-green-roofs","title":"Step 6: Spatial Distribution of Green Roofs","text":"<p>Then, we use the 2D Plot (Vega-Lite) node to create a dot density map to explore the spatial distribution of green roofs. Each dot represents a green roof, with its position determined by latitude and longitude, and its size encoding the green roof area.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"description\": \"Dot Density Map of Green Roof Locations in Chicago with Zoom &amp; Pan\",\n  \"width\": 500,\n  \"height\": 600,\n  \"title\": \"Green Roof Locations in Chicago\",\n  \"mark\": \"circle\",\n  \"selection\": {\n    \"grid\": {\n      \"type\": \"interval\",\n      \"bind\": \"scales\"\n    }\n  },\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"LONGITUDE\",\n      \"type\": \"quantitative\",\n      \"scale\": { \"domain\": [-88.0, -87.5] },\n      \"axis\": { \"title\": \"Longitude\" }\n    },\n    \"y\": {\n      \"field\": \"LATITUDE\",\n      \"type\": \"quantitative\",\n      \"scale\": { \"domain\": [41.6, 42.1] },\n      \"axis\": { \"title\": \"Latitude\" }\n    },\n    \"size\": {\n      \"field\": \"VEGETATED_SQFT\",\n      \"type\": \"quantitative\",\n      \"legend\": { \"title\": \"Vegetated Sqft\" }\n    },\n    \"tooltip\": [\n      { \"field\": \"VEGETATED_SQFT\", \"type\": \"quantitative\" },\n      { \"field\": \"TOTAL_ROOF_SQFT\", \"type\": \"quantitative\" },\n      { \"field\": \"zip\", \"type\": \"nominal\" }\n    ]\n  },\n  \"config\": {\n    \"view\": { \"stroke\": \"transparent\" }\n  }\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/10-green-roofs/#step-7-data-transformation","title":"Step 7: Data Transformation","text":"<p>Then, we create a Data Transformation node and connect it to the same Data Pool node as before. This node will be used to calculate the top 10 zip codes with the largest green roofs.</p> <pre><code>import geopandas as gpd\n\njoined = arg\n\n# filter out the top 10 zip codes from the joined dataframe on 'zip' by square feet\ntop_10_largest = joined.groupby('zip')['VEGETATED_SQFT'].sum().reset_index().sort_values(by='VEGETATED_SQFT', ascending=False).head(10)\n\nreturn top_10_largest\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/10-green-roofs/#step-8-visualizing-zip-codes-with-largest-green-roofs","title":"Step 8: Visualizing Zip Codes with Largest Green Roofs","text":"<p>Now, we create a 2D Plot (Vega-Lite) node to create a bar chart that visualizes the top 10 zip codes with the largest green roof areas.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"description\": \"Top 10 largest zip codes by green roof area\",\n  \"width\": 400,\n  \"height\": 200,\n  \"selection\": {\n    \"zip_select\": {\n      \"type\": \"multi\",\n      \"fields\": [\"zip\"],\n      \"toggle\": \"event.shiftKey\"\n    }\n  },\n  \"mark\": { \"type\": \"bar\", \"stroke\": \"black\", \"color\": \"green\" },\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"VEGETATED_SQFT\",\n      \"type\": \"quantitative\",\n      \"axis\": {\n        \"title\": \"Total Vegetated Roof Size (sqft)\",\n        \"values\": [100000, 250000, 500000, 1000000],\n        \"format\": \",d\"\n      }\n    },\n    \"y\": {\n      \"field\": \"zip\",\n      \"type\": \"nominal\",\n      \"sort\": \"-x\",\n      \"axis\": { \"title\": \"Zip Code\" }\n    },\n    \"color\": {\n      \"field\": \"zip\",\n      \"type\": \"nominal\",\n      \"scale\": { \"scheme\": \"category20\" }\n    },\n    \"opacity\": {\n      \"condition\": { \"selection\": \"zip_select\", \"value\": 1 },\n      \"value\": 0.3\n    },\n    \"tooltip\": [\n      { \"field\": \"zip\", \"type\": \"nominal\" },\n      { \"field\": \"VEGETATED_SQFT\", \"type\": \"quantitative\" }\n    ]\n  }\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/10-green-roofs/#final-result","title":"Final result","text":"<p>This example demonstrates how Curio can be used for spatial analysis of urban sustainability initiatives. By integrating tabular green roof data with geospatial boundaries, this workflow generates interactive visualizations like dot density maps and bar charts. These tools enable a deeper understanding of spatial patterns, size distributions, and key neighborhoods with high concentrations of green infrastructure.</p>"},{"location":"examples/detailed_examples/11-building-energy/","title":"Example: Visual analytics of building energy usage","text":"<p>Authors: Adam Shaar, Anurag Reddy, Sathvika K., Aakash Kolli</p> <p>In this example, we will explore how Curio can be used to analyze and visualize building energy consumption in Chicago. By integrating and transforming energy usage data, we will analyze how electricity and gas consumption varies across building types, neighborhoods, time periods, and building characteristics such as age and height.</p> <p>Here is the overview of the entire dataflow pipeline:</p> <p> </p> <p>Before you begin, please familiarize yourself with Curio\u2019s main concepts and functionalities by reading our usage guide.</p> <p>The data for this tutorial can be found here.</p> <p>For completeness, we also include the template code in each dataflow step.</p>"},{"location":"examples/detailed_examples/11-building-energy/#step-1-dataflow-to-load-and-clean-building-energy-data","title":"Step 1: Dataflow to load and clean building energy data","text":"<p>We begin by creating a Data Loading node to import the building energy usage dataset. This step reads the CSV file and selects relevant columns for further analysis.</p> <pre><code>import pandas as pd\n\n# Load the CSV directly\ndf = pd.read_csv(\"Energy_Usage_5000../data/../data/.csv\")\n\n# Select relevant columns and clean missing values\ngrouped_data = df[[\"BUILDING TYPE\", \"TOTAL KWH\", \"TOTAL THERMS\"]].dropna()\n\n# Return cleaned DataFrame\nreturn grouped_data\n</code></pre> <p></p> <p>Next, we create a Data Cleaning node and connect it to the previous node. This node removes outliers, fills missing values, and standardizes key columns for analysis.</p> <pre><code>def remove_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    return df[(df[column] &gt;= Q1 - 1.5 * IQR) &amp; (df[column] &lt;= Q3 + 1.5 * IQR)]\n\ndef clean(df):\n    # Drop only if columns exist\n    required_cols = ['CENSUS BLOCK', 'BUILDING TYPE', 'BUILDING_SUBTYPE']\n    drop_cols = [col for col in required_cols if col in df.columns]\n\n    df_cleaned = df.dropna(subset=drop_cols).copy()\n\n    # Standard KWH/THERM fill\n    kwh_columns = [col for col in df.columns if 'KWH' in col and '2010' in col and 'SQFT' not in col]\n    therm_columns = [col for col in df.columns if 'THERM' in col and '2010' in col and 'SQFT' not in col]\n    df_cleaned[kwh_columns] = df_cleaned[kwh_columns].fillna(df_cleaned[kwh_columns].median())\n    df_cleaned[therm_columns] = df_cleaned[therm_columns].fillna(df_cleaned[therm_columns].median())\n\n    for col in [\n        'TOTAL KWH', 'TOTAL THERMS',\n        'OCCUPIED UNITS PERCENTAGE', 'OCCUPIED UNITS',\n        'RENTER-OCCUPIED HOUSING UNITS'\n    ]:\n        if col in df_cleaned.columns:\n            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n\n    df_cleaned['ELECTRICITY ACCOUNTS'] = pd.to_numeric(df_cleaned.get('ELECTRICITY ACCOUNTS'), errors='coerce')\n    df_cleaned['GAS ACCOUNTS'] = pd.to_numeric(df_cleaned.get('GAS ACCOUNTS'), errors='coerce')\n    df_cleaned['ELECTRICITY'] = df_cleaned['TOTAL KWH']\n    df_cleaned['GAS'] = df_cleaned['TOTAL THERMS']\n\n    df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() &lt; 0.2]\n\n    if 'TERM APRIL 2010' in df.columns:\n        df.rename(columns={'TERM APRIL 2010': 'THERM APRIL 2010'}, inplace=True)\n\n    # Standardize community names\n    if 'COMMUNITY AREA NAME' in df_cleaned.columns:\n        df_cleaned['COMMUNITY AREA NAME'] = df_cleaned['COMMUNITY AREA NAME'].str.strip().str.upper()\n        df_cleaned['COMMUNITY AREA NAME'] = df_cleaned['COMMUNITY AREA NAME'].replace({\n            \"LAKEVIEW\": \"LAKE VIEW\",\n            \"O'HARE\": \"OHARE\"\n        })\n\n    # Ensure total columns are present\n    if 'TOTAL KWH' in df_cleaned.columns:\n        df_cleaned['TOTAL KWH'] = df_cleaned['TOTAL KWH'].fillna(df_cleaned['TOTAL KWH'].median())\n    if 'TOTAL THERMS' in df_cleaned.columns:\n        df_cleaned['TOTAL THERMS'] = df_cleaned['TOTAL THERMS'].fillna(df_cleaned['TOTAL THERMS'].median())\n\n    if 'AVERAGE BUILDING AGE' in df_cleaned.columns:\n        df_cleaned['DECADE BUILT'] = (2010 - df_cleaned['AVERAGE BUILDING AGE']) // 10 * 10\n\n    df_cleaned = remove_outliers(df_cleaned, 'TOTAL KWH')\n    df_cleaned = remove_outliers(df_cleaned, 'TOTAL THERMS')\n\n\n\n    return df_cleaned\n\n\n# Run cleaning and return\nreturn clean(arg)\n</code></pre> <p>Now, we create a Data Transformation node and attach it to the Data Cleaning node. This node reshapes the data to compare electricity and gas usage across building types, and calculates the percentage contribution of each energy type.</p> <pre><code># Assume `arg` is the cleaned DataFrame from the previous card\nimport pandas as pd\n\nenergy_long = pd.melt(\n    arg,\n    id_vars='BUILDING TYPE',\n    value_vars=['TOTAL KWH', 'TOTAL THERMS'],\n    var_name='ENERGY TYPE',\n    value_name='VALUE'\n)\n\ntotal_by_type = energy_long.groupby('BUILDING TYPE')['VALUE'].transform('sum')\nenergy_long['PERCENTAGE'] = (energy_long['VALUE'] / total_by_type) * 100\n\nreturn energy_long\n</code></pre> <p></p> <p>Next, we use a 2D Plot (Vega-Lite) node to create a heatmap showing the distribution of energy consumption by building type and energy type. This visualization helps us quickly identify which building types consume the most energy and the relative proportions of electricity and gas.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"energy_transformed_1\" },\n  \"mark\": \"rect\",\n  \"encoding\": {\n    \"x\": { \"field\": \"BUILDING TYPE\", \"type\": \"nominal\" },\n    \"y\": { \"field\": \"ENERGY TYPE\", \"type\": \"nominal\" },\n    \"color\": {\n      \"field\": \"VALUE\",\n      \"type\": \"quantitative\",\n      \"scale\": { \"scheme\": \"viridis\" }\n    },\n    \"tooltip\": [\n      { \"field\": \"BUILDING TYPE\" },\n      { \"field\": \"ENERGY TYPE\" },\n      { \"field\": \"VALUE\", \"format\": \".2f\" },\n      { \"field\": \"PERCENTAGE\", \"format\": \".1f\" }\n    ]\n  },\n  \"title\": \"Energy Consumption Heatmap (KWH + THERMS)\"\n}\n</code></pre> <p></p> <p>We then create another a 2D Plot (Vega-Lite) node to visualize the distribution of energy usage by building type and energy type as a dot plot. This provides another perspective on the data, highlighting the magnitude of usage for each category.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"energy_transformed_1\" },\n  \"mark\": \"circle\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"BUILDING TYPE\",\n      \"type\": \"nominal\",\n      \"axis\": { \"labelAngle\": -45 }\n    },\n    \"y\": { \"field\": \"VALUE\", \"type\": \"quantitative\" },\n    \"color\": { \"field\": \"ENERGY TYPE\", \"type\": \"nominal\" },\n    \"size\": { \"field\": \"VALUE\", \"type\": \"quantitative\" },\n    \"tooltip\": [\n      { \"field\": \"BUILDING TYPE\" },\n      { \"field\": \"ENERGY TYPE\" },\n      { \"field\": \"VALUE\", \"format\": \".2f\" }\n    ]\n  },\n  \"title\": \"Dot Plot of Energy Usage by Building Type\"\n}\n</code></pre> <p></p> <p>Next, we create another Data Transformation node and attach it to the same Data Cleaning node as before. This node calculates the average gas usage for each building type, which is useful for understanding which types of buildings are the largest consumers of gas.</p> <pre><code># Group by building type and compute average gas usage\ndf_avg_gas = arg.groupby(\"BUILDING TYPE\")[\"TOTAL THERMS\"].mean().reset_index()\ndf_avg_gas.rename(columns={\"TOTAL THERMS\": \"AVG TOTAL THERMS\"}, inplace=True)\n\nreturn df_avg_gas\n</code></pre> <p></p> <p>We then use a 2D Plot (Vega-Lite) node to create a bar chart visualizing the average gas usage by building type. This chart makes it easy to compare gas consumption across different building categories.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"avg_gas_by_building\" },\n  \"mark\": \"bar\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"BUILDING TYPE\",\n      \"type\": \"nominal\",\n      \"axis\": { \"labelAngle\": -45 }\n    },\n    \"y\": { \"field\": \"AVG TOTAL THERMS\", \"type\": \"quantitative\" },\n    \"tooltip\": [\n      { \"field\": \"BUILDING TYPE\" },\n      { \"field\": \"AVG TOTAL THERMS\", \"format\": \".2f\" }\n    ],\n    \"color\": { \"field\": \"BUILDING TYPE\", \"type\": \"nominal\" }\n  }\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/11-building-energy/#step-2-dataflow-to-analyze-community-level-energy-consumption-and-building-type-patterns","title":"Step 2: Dataflow to analyze community-level energy consumption and building type patterns","text":"<p>We create a Data Loading node to import the dataset for community-level analysis. This node loads the full dataset for further cleaning and aggregation.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Energy_Usage_5000../data/../data/.csv\")\n\n# Standardize column names right away for consistency\ndf.columns = [col.upper().strip() for col in df.columns]\n\n# Just return full dataset for now no filtering yet\nreturn df\n</code></pre> <p></p> <p>Next, we create a Data Cleaning node to remove missing values and outliers, and to standardize community names. This ensures the data is consistent and reliable for community-level comparisons.</p> <pre><code>def remove_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    return df[(df[column] &gt;= Q1 - 1.5 * IQR) &amp; (df[column] &lt;= Q3 + 1.5 * IQR)]\n\ndef clean(df):\n    # We assume column names are already uppercased by the data loading card\n    required_cols = ['COMMUNITY AREA NAME', 'TOTAL KWH', 'TOTAL THERMS', 'BUILDING TYPE']\n    df = df.dropna(subset=required_cols).copy()\n\n    df['COMMUNITY AREA NAME'] = df['COMMUNITY AREA NAME'].str.strip().str.upper()\n    df['TOTAL KWH'] = df['TOTAL KWH'].fillna(df['TOTAL KWH'].median())\n    df['TOTAL THERMS'] = df['TOTAL THERMS'].fillna(df['TOTAL THERMS'].median())\n\n    df = remove_outliers(df, 'TOTAL KWH')\n    df = remove_outliers(df, 'TOTAL THERMS')\n\n    return df\n\nreturn clean(arg)\n</code></pre> <p>Now, we create a Data Transformation node and attach it to the Data Cleaning node. This node computes the top 10 communities by average energy use, aggregating both electricity and gas consumption.</p> <pre><code>df_avg = arg[[\"COMMUNITY AREA NAME\", \"TOTAL KWH\", \"TOTAL THERMS\"]].dropna()\n\nagg_df = df_avg.groupby(\"COMMUNITY AREA NAME\").agg({\n    \"TOTAL KWH\": \"mean\",\n    \"TOTAL THERMS\": \"mean\"\n}).reset_index()\n\nagg_df[\"AVG ENERGY USE\"] = agg_df[\"TOTAL KWH\"] + agg_df[\"TOTAL THERMS\"]\n\ntop10 = agg_df.sort_values(\"AVG ENERGY USE\", ascending=False).head(10)\n\nreturn top10\n</code></pre> <p></p> <p>We then use a 2D Plot (Vega-Lite) node to create a bar chart of the top 10 communities by average energy consumption. This visualization highlights which communities have the highest energy demands.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"top10_avg_energy_by_community\" },\n  \"mark\": \"bar\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"COMMUNITY AREA NAME\",\n      \"type\": \"nominal\",\n      \"sort\": \"-y\",\n      \"axis\": { \"labelAngle\": -45 }\n    },\n    \"y\": {\n      \"field\": \"AVG ENERGY USE\",\n      \"type\": \"quantitative\",\n      \"title\": \"Avg Energy Use (KWH + THERMS)\"\n    },\n    \"tooltip\": [\n      { \"field\": \"COMMUNITY AREA NAME\" },\n      { \"field\": \"AVG ENERGY USE\", \"format\": \".2f\" }\n    ],\n    \"color\": {\n      \"field\": \"AVG ENERGY USE\",\n      \"type\": \"quantitative\",\n      \"scale\": { \"scheme\": \"blues\" }\n    }\n  },\n  \"title\": \"Top 10 Communities by Avg Energy Consumption\"\n}\n</code></pre> <p>Next, we create another Data Transformation node and attach it to the same Data Cleaning node. This node prepares the data for a scatter plot comparing electricity and gas usage by building type, filtering out any zero or missing values.</p> <pre><code>df_scatter = arg[[\"TOTAL KWH\", \"TOTAL THERMS\", \"BUILDING TYPE\"]].dropna()\ndf_scatter = df_scatter[(df_scatter[\"TOTAL KWH\"] &gt; 0) &amp; (df_scatter[\"TOTAL THERMS\"] &gt; 0)]\n\nreturn df_scatter\n</code></pre> <p></p> <p>We then use a 2D Plot (Vega-Lite) node to create a scatter plot of electricity vs gas usage, colored by building type. This helps us explore the relationship between the two forms of energy consumption across building types.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"scatter_energy_usage\" },\n  \"mark\": \"point\",\n  \"encoding\": {\n    \"x\": {\n      \"field\": \"TOTAL KWH\",\n      \"type\": \"quantitative\",\n      \"scale\": { \"type\": \"log\" }\n    },\n    \"y\": {\n      \"field\": \"TOTAL THERMS\",\n      \"type\": \"quantitative\",\n      \"scale\": { \"type\": \"log\" }\n    },\n    \"color\": { \"field\": \"BUILDING TYPE\", \"type\": \"nominal\" },\n    \"tooltip\": [\n      { \"field\": \"BUILDING TYPE\" },\n      { \"field\": \"TOTAL KWH\" },\n      { \"field\": \"TOTAL THERMS\" }\n    ]\n  },\n  \"title\": \"Electricity vs Gas Usage by Building Type (Log Scale)\"\n}\n</code></pre> <p>Now, we create a third Data Transformation node and attach it to the same Data Cleaning node. This node prepares the data for a strip plot of gas usage by building type, removing large outliers for clarity.</p> <pre><code>df_strip = arg[[\"BUILDING TYPE\", \"TOTAL THERMS\"]].dropna()\n\n# Remove large outliers for visualization clarity\ndf_strip = df_strip[df_strip[\"TOTAL THERMS\"] &lt; 500_000]\n\nreturn df_strip\n</code></pre> <p></p> <p>Finally, we use a 2D Plot (Vega-Lite) node to create a strip plot showing the spread of gas usage by building type. This visualization reveals the distribution and variability of gas consumption within each building category.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"data\": { \"name\": \"df_strip\" },\n  \"mark\": \"tick\",\n  \"encoding\": {\n    \"x\": { \"field\": \"TOTAL THERMS\", \"type\": \"quantitative\" },\n    \"y\": { \"field\": \"BUILDING TYPE\", \"type\": \"nominal\" },\n    \"color\": { \"field\": \"BUILDING TYPE\", \"type\": \"nominal\" },\n    \"tooltip\": [{ \"field\": \"BUILDING TYPE\" }, { \"field\": \"TOTAL THERMS\" }]\n  },\n  \"title\": \"Gas Usage Spread by Building Type (Strip Plot)\"\n}\n</code></pre>"},{"location":"examples/detailed_examples/11-building-energy/#step-3-dataflow-to-explore-monthly-electricity-usage-trends-by-community","title":"Step 3: Dataflow to explore monthly electricity usage trends by community","text":"<p>We create a Data Loading node to</p> <pre><code>import pandas as pd\ndf = pd.read_csv(\"Energy_Usage_5000../data/../data/.csv\")\nreturn df\n</code></pre> <p></p> <p>Next, we create a Data Cleaning node to reshape the data into a long format, extracting monthly KWH usage for each community. This step prepares the data for time series analysis.</p> <pre><code># Filter for KWH month columns\nmonth_cols = [col for col in arg.columns if col.startswith(\"KWH \") and \"2010\" in col]\nrequired_cols = [\"COMMUNITY AREA NAME\"] + month_cols\n\ndf = arg[required_cols].dropna()\n\n# Melt to long format\ndf_long = pd.melt(\n    df,\n    id_vars=[\"COMMUNITY AREA NAME\"],\n    value_vars=month_cols,\n    var_name=\"Month\",\n    value_name=\"KWH\"\n)\n\n# Extract month name (e.g., \"JANUARY\")\ndf_long[\"Month\"] = df_long[\"Month\"].str.extract(r\"KWH (.+?) 2010\")[0].str.upper()\ndf_long = df_long.dropna(subset=[\"Month\", \"KWH\", \"COMMUNITY AREA NAME\"])\n\nreturn df_long\n</code></pre> <p>Now, we create a Data Transformation node and attach it to the Data Cleaning node. This node selects the top 20 communities by average KWH usage, focusing the analysis on the highest-consuming areas.</p> <pre><code># Get top 20 communities by average KWH\ntop_20 = arg.groupby(\"COMMUNITY AREA NAME\")[\"KWH\"].mean().sort_values(ascending=False).head(20).index\n\n# Filter the long-form data\ndf_top20 = arg[arg[\"COMMUNITY AREA NAME\"].isin(top_20)].copy()\nreturn df_top20\n</code></pre> <p>Lastly, we use a 2D Plot (Vega-Lite) node to ...</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"params\": [\n    {\n      \"name\": \"commPick\",\n      \"select\": {\n        \"type\": \"point\",\n        \"fields\": [\"COMMUNITY AREA NAME\"]\n      }\n    }\n  ],\n  \"vconcat\": [\n    {\n      \"title\": \"Click on a Line to Highlight a Community\",\n      \"width\": 650,\n      \"height\": 400,\n      \"mark\": {\n        \"type\": \"line\",\n        \"interpolate\": \"monotone\"\n      },\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"Month\",\n          \"type\": \"nominal\",\n          \"sort\": [\n            \"JANUARY\",\n            \"FEBRUARY\",\n            \"MARCH\",\n            \"APRIL\",\n            \"MAY\",\n            \"JUNE\",\n            \"JULY\",\n            \"AUGUST\",\n            \"SEPTEMBER\",\n            \"OCTOBER\",\n            \"NOVEMBER\",\n            \"DECEMBER\"\n          ],\n          \"axis\": { \"labelAngle\": 45 }\n        },\n        \"y\": {\n          \"field\": \"KWH\",\n          \"type\": \"quantitative\",\n          \"title\": \"Total KWH\",\n          \"scale\": { \"zero\": false }\n        },\n        \"color\": {\n          \"field\": \"COMMUNITY AREA NAME\",\n          \"type\": \"nominal\",\n          \"scale\": { \"scheme\": \"category20\" },\n          \"legend\": { \"columns\": 2 }\n        },\n        \"opacity\": {\n          \"condition\": { \"param\": \"commPick\", \"value\": 1 },\n          \"value\": 0.2\n        },\n        \"tooltip\": [\n          { \"field\": \"COMMUNITY AREA NAME\", \"title\": \"Community\" },\n          { \"field\": \"Month\" },\n          { \"field\": \"KWH\", \"format\": \",.0f\" }\n        ]\n      }\n    },\n    {\n      \"title\": \"Average KWH of Selected Community\",\n      \"width\": 650,\n      \"height\": 300,\n      \"mark\": \"bar\",\n      \"encoding\": {\n        \"y\": {\n          \"field\": \"COMMUNITY AREA NAME\",\n          \"type\": \"nominal\",\n          \"sort\": \"-x\"\n        },\n        \"x\": {\n          \"aggregate\": \"mean\",\n          \"field\": \"KWH\",\n          \"type\": \"quantitative\",\n          \"title\": \"Avg KWH\"\n        },\n        \"color\": {\n          \"field\": \"COMMUNITY AREA NAME\",\n          \"type\": \"nominal\"\n        }\n      },\n      \"transform\": [{ \"filter\": { \"param\": \"commPick\" } }]\n    }\n  ]\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/11-building-energy/#step-4-dataflow-to-analyze-monthly-energy-usage-by-community-and-month","title":"Step 4: Dataflow to analyze monthly energy usage by community and month","text":"<p>We create a Data Loading node to import the dataset, followed by a Data Cleaning node to reshape the data for monthly analysis by community. This prepares the data for interactive exploration of energy usage patterns over time.</p> <p></p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Energy_Usage_5000../data/../data/.csv\")\n\nmonth_cols = [col for col in df.columns if col.startswith(\"KWH \") and \"2010\" in col]\nrequired_cols = [\"COMMUNITY AREA NAME\"] + month_cols\n\ndf = df[required_cols].dropna()\nreturn df\n</code></pre> <p></p> <p>Next, we create a Data Cleaning node to convert the data to long format, extracting monthly KWH usage for each community. This step is essential for visualizing trends across months and communities.</p> <pre><code>df_long = pd.melt(\n    arg,\n    id_vars=[\"COMMUNITY AREA NAME\"],\n    value_vars=[col for col in arg.columns if \"KWH\" in col],\n    var_name=\"Month\",\n    value_name=\"KWH\"\n)\n\ndf_long[\"Month\"] = df_long[\"Month\"].str.extract(r\"KWH (.+?) 2010\")[0].str.upper()\ndf_long = df_long.dropna(subset=[\"Month\", \"KWH\", \"COMMUNITY AREA NAME\"])\n\nreturn df_long\n</code></pre> <p>Now, we create a Data Transformation node and attach it to the Data Cleaning node. This node selects the top 20 communities by average KWH usage, narrowing the focus to the most energy-intensive areas.</p> <pre><code>top20_names = arg.groupby(\"COMMUNITY AREA NAME\")[\"KWH\"].mean().sort_values(ascending=False).head(20).index\ndf_top20 = arg[arg[\"COMMUNITY AREA NAME\"].isin(top20_names)].copy()\n\nreturn df_top20\n</code></pre> <p>Lastly, we use a 2D Plot (Vega-Lite) node to create an interactive bar chart showing average KWH by community, filtered by selected months. This enables dynamic exploration of how energy usage varies throughout the year.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"vconcat\": [\n    {\n      \"title\": \"Monthly Average Energy Usage (Brush to Select Months)\",\n      \"params\": [\n        {\n          \"name\": \"monthBrush\",\n          \"select\": {\n            \"type\": \"interval\",\n            \"encodings\": [\"x\"]\n          }\n        }\n      ],\n      \"mark\": \"bar\",\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"Month\",\n          \"type\": \"ordinal\",\n          \"scale\": {\n            \"domain\": [\n              \"JANUARY\",\n              \"FEBRUARY\",\n              \"MARCH\",\n              \"APRIL\",\n              \"MAY\",\n              \"JUNE\",\n              \"JULY\",\n              \"AUGUST\",\n              \"SEPTEMBER\",\n              \"OCTOBER\",\n              \"NOVEMBER\",\n              \"DECEMBER\"\n            ]\n          },\n          \"axis\": {\n            \"labelAngle\": -40,\n            \"labelFontSize\": 11\n          }\n        },\n        \"y\": {\n          \"aggregate\": \"mean\",\n          \"field\": \"KWH\",\n          \"type\": \"quantitative\",\n          \"title\": \"Avg KWH\"\n        },\n        \"tooltip\": [\n          { \"field\": \"Month\" },\n          { \"aggregate\": \"mean\", \"field\": \"KWH\", \"title\": \"Avg KWH\" }\n        ],\n        \"color\": {\n          \"value\": \"#4C78A8\"\n        }\n      }\n    },\n    {\n      \"title\": \"Avg KWH by Community (Filtered by Selected Months)\",\n      \"transform\": [\n        {\n          \"filter\": { \"param\": \"monthBrush\" }\n        }\n      ],\n      \"mark\": \"bar\",\n      \"encoding\": {\n        \"y\": {\n          \"field\": \"COMMUNITY AREA NAME\",\n          \"type\": \"nominal\",\n          \"sort\": \"-x\"\n        },\n        \"x\": {\n          \"aggregate\": \"mean\",\n          \"field\": \"KWH\",\n          \"type\": \"quantitative\",\n          \"title\": \"Avg KWH\"\n        },\n        \"color\": {\n          \"field\": \"COMMUNITY AREA NAME\",\n          \"type\": \"nominal\"\n        },\n        \"tooltip\": [\n          { \"field\": \"COMMUNITY AREA NAME\" },\n          { \"aggregate\": \"mean\", \"field\": \"KWH\", \"title\": \"Avg KWH\" }\n        ]\n      }\n    }\n  ]\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/11-building-energy/#step-5-dataflow-to-examine-energy-usage-by-building-age-and-stories","title":"Step 5: Dataflow to examine energy usage by building age and stories","text":"<p>We create a Data Loading node to import the dataset, followed by a Data Cleaning node to categorize buildings by age and number of stories. This step enables analysis of how building characteristics relate to energy consumption.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Energy_Usage_5000../data/../data/.csv\")\n\ncolumns_needed = [\"AVERAGE STORIES\", \"AVERAGE BUILDING AGE\", \"TOTAL KWH\"] + [col for col in df.columns if col.startswith(\"KWH \") and \"2010\" in col]\n\ndf = df[columns_needed].dropna()\nreturn df\n</code></pre> <p></p> <p>Next, we create a Data Cleaning node to create categorical brackets for building age and stories, making it easier to compare groups of buildings.</p> <pre><code>def story_bracket(stories):\n    if stories &lt;= 1:\n        return \"1 story\"\n    elif stories == 2:\n        return \"2 stories\"\n    elif 3 &lt;= stories &lt;= 5:\n        return \"3-5 stories\"\n    elif 6 &lt;= stories &lt;= 10:\n        return \"6-10 stories\"\n    else:\n        return \"11+ stories\"\n\ndef age_bracket(age):\n    if age &lt;= 20:\n        return \"0-20 yrs\"\n    elif age &lt;= 40:\n        return \"21-40 yrs\"\n    elif age &lt;= 60:\n        return \"41-60 yrs\"\n    elif age &lt;= 80:\n        return \"61-80 yrs\"\n    else:\n        return \"81+ yrs\"\n\ndf = arg.copy()\n\ndf[\"STORY BRACKET\"] = df[\"AVERAGE STORIES\"].apply(story_bracket)\ndf[\"AGE BRACKET\"] = df[\"AVERAGE BUILDING AGE\"].apply(age_bracket)\nreturn df\n</code></pre> <p>Now, we create a Data Transformation node and attach it to the Data Cleaning node. This node reshapes the data for analysis by age and stories, preparing it for visualization.</p> <pre><code>import pandas as pd\n\ndf_long = pd.melt(\n    arg,\n    id_vars=[\"STORY BRACKET\", \"AGE BRACKET\", \"TOTAL KWH\"],\n    value_vars=[col for col in arg.columns if col.startswith(\"KWH \")],\n    var_name=\"Month\",\n    value_name=\"KWH\"\n)\n\ndf_long[\"Month\"] = df_long[\"Month\"].str.extract(r\"KWH (.+?) 2010\")[0].str.upper()\ndf_long = df_long.dropna(subset=[\"Month\", \"KWH\", \"STORY BRACKET\", \"AGE BRACKET\"])\n\nreturn df_long\n</code></pre> <p>Lastly, we use a 2D Plot (Vega-Lite) node to create a box plot and line chart showing the distribution and trends of KWH usage by building age and stories. These visualizations help us understand how energy consumption varies with building characteristics and over time.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"params\": [\n    {\n      \"name\": \"storySelect\",\n      \"bind\": {\n        \"input\": \"select\",\n        \"options\": [\n          \"1 story\",\n          \"2 stories\",\n          \"3-5 stories\",\n          \"6-10 stories\",\n          \"11+ stories\"\n        ]\n      },\n      \"value\": \"1 story\"\n    }\n  ],\n  \"vconcat\": [\n    {\n      \"width\": 600,\n      \"title\": {\n        \"text\": \"Distribution of Total KWH by Age (Box Plot)\",\n        \"align\": \"center\"\n      },\n      \"transform\": [{ \"filter\": \"datum['STORY BRACKET'] == storySelect\" }],\n      \"mark\": \"boxplot\",\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"AGE BRACKET\",\n          \"type\": \"nominal\",\n          \"sort\": [\"0-20 yrs\", \"21-40 yrs\", \"41-60 yrs\", \"61-80 yrs\", \"81+ yrs\"]\n        },\n        \"y\": {\n          \"field\": \"TOTAL KWH\",\n          \"type\": \"quantitative\",\n          \"title\": \"Total KWH\"\n        },\n        \"color\": {\n          \"field\": \"AGE BRACKET\",\n          \"type\": \"nominal\",\n          \"legend\": {\n            \"orient\": \"right\",\n            \"anchor\": \"middle\",\n            \"direction\": \"vertical\"\n          }\n        },\n        \"tooltip\": [{ \"field\": \"AGE BRACKET\" }, { \"field\": \"TOTAL KWH\" }]\n      }\n    },\n    {\n      \"width\": 600,\n      \"title\": {\n        \"text\": \"Monthly Avg KWH Trend by Age (for Selected Stories)\",\n        \"align\": \"center\"\n      },\n      \"transform\": [{ \"filter\": \"datum['STORY BRACKET'] == storySelect\" }],\n      \"mark\": { \"type\": \"line\", \"point\": true },\n      \"encoding\": {\n        \"x\": {\n          \"field\": \"Month\",\n          \"type\": \"ordinal\",\n          \"sort\": [\n            \"JANUARY\",\n            \"FEBRUARY\",\n            \"MARCH\",\n            \"APRIL\",\n            \"MAY\",\n            \"JUNE\",\n            \"JULY\",\n            \"AUGUST\",\n            \"SEPTEMBER\",\n            \"OCTOBER\",\n            \"NOVEMBER\",\n            \"DECEMBER\"\n          ]\n        },\n        \"y\": {\n          \"aggregate\": \"mean\",\n          \"field\": \"KWH\",\n          \"type\": \"quantitative\",\n          \"title\": \"Avg Monthly KWH\"\n        },\n        \"color\": {\n          \"field\": \"AGE BRACKET\",\n          \"type\": \"nominal\",\n          \"legend\": {\n            \"orient\": \"right\",\n            \"anchor\": \"middle\",\n            \"direction\": \"vertical\"\n          }\n        },\n        \"tooltip\": [\n          { \"field\": \"Month\" },\n          { \"aggregate\": \"mean\", \"field\": \"KWH\" },\n          { \"field\": \"AGE BRACKET\" }\n        ]\n      }\n    }\n  ],\n  \"config\": {\n    \"concat\": { \"align\": \"center\" }\n  }\n}\n</code></pre> <p></p>"},{"location":"examples/detailed_examples/11-building-energy/#final-result","title":"Final result","text":"<p>This workflow uses interactive visualizations to explore electricity and gas consumption by building type, neighborhood, season, and building attributes:</p> <ol> <li>Heatmaps and dot plots compare energy use across building types.</li> <li>Bar, scatter, and strip plots rank neighborhoods and building types by energy demand.</li> <li>Interactive line and bar charts show seasonal electricity use by community.</li> <li>Monthly bar charts show changes in average electricity use by neighborhood over time.</li> <li>Box plots and line charts relate energy use to building age and number of stories.</li> </ol> <p>These visualizations show patterns and trends that can guide energy efficiency improvements. Curio\u2019s modular framework allows users to combine multiple visualizations and datasets to build rich, interactive analyses of urban data.</p>"},{"location":"getting-started/installation/","title":"\ud83d\ude80 Installation Guide","text":"<p>This guide will help you get Curio up and running on your system. Curio includes a multi-server management tool that orchestrates three key components: the Backend for provenance tracking and user management, the Sandbox for executing code modules, and the Frontend for building visual workflows.</p> <p>System Requirements</p> <ul> <li>Python &gt;= 3.10 &amp; &lt; 3.12</li> <li>Tested on Windows 11, macOS Sonoma 14.5, and Ubuntu</li> <li>Docker (for Docker installation method)</li> </ul>"},{"location":"getting-started/installation/#installation-overview","title":"\ud83c\udfaf Installation Overview","text":"<p>The <code>curio</code> launcher is a unified command-line tool for starting, stopping, and rebuilding the various Curio servers. Depending on your installation method, you'll use either:</p> <ul> <li><code>curio</code> command (if installed via pip)</li> <li><code>python curio.py</code> command (if installed from Git repository)</li> </ul> <p>You can inspect the help message by running:</p> Via pipFrom Git <pre><code>curio --help\n</code></pre> <pre><code>python curio.py --help\n</code></pre> <p>Sample output: <pre><code>Usage:\n  curio start                       # Start all servers (Backend, Sandbox, Frontend)\n  curio start backend               # Start only the backend (localhost:5002)\n  curio start sandbox               # Start only the sandbox (localhost:2000)\n  curio start --verbose VERBOSE     # Verbosity level (e.g., 0=silent, 1=normal, 2=debug)\n  curio start --force-rebuild       # Re-build the frontend and start all servers\n  curio start --force-db-init       # Re-initialize the backend database and start all servers\n</code></pre></p>"},{"location":"getting-started/installation/#quick-installation-recommended","title":"\u26a1 Quick Installation (Recommended)","text":"<p>The fastest way to get started with Curio:</p> <pre><code># Install Curio\npip install utk-curio\n\n# Start all components\ncurio start\n</code></pre> <p>That's it!</p> <p>This will start the backend, sandbox, and frontend servers. The first time Curio runs, it will automatically install UTK. Curio's frontend will be available at http://localhost:8080.</p>"},{"location":"getting-started/installation/#installation-methods","title":"\ud83d\udce6 Installation Methods","text":"<p>Choose the installation method that best fits your needs:</p> Via pip (Quick Setup)Via Docker (Containerized)Manual Installation (Development) <p>Perfect for: Quick setup and general usage</p> <pre><code>pip install utk-curio\n</code></pre> <p>Starting Curio: <pre><code># Start all components\ncurio start\n\n# Or start components individually\ncurio start backend\ncurio start sandbox\ncurio start frontend\n</code></pre></p> <p>Frontend Limitations</p> <p>The pip installation includes a pre-built frontend and does not support rebuilding it. If you need to modify or rebuild the frontend, please use the manual installation method.</p> <p>Perfect for: Easy deployment and consistent environments</p> <p>Prerequisites: Docker</p> <pre><code># Clone the repository\ngit clone git@github.com:urban-toolkit/curio.git\ncd curio\n\n# Start with Docker Compose\ndocker compose up\n</code></pre> <p>For older Docker versions: <pre><code>docker-compose up\n</code></pre></p> <p>Docker Notes</p> <ul> <li>Initial builds can take time</li> <li>Use <code>--build</code> flag to rebuild if needed</li> <li>Curio's frontend will be available at http://localhost:8080</li> </ul> <p>Perfect for: Development, customization, and frontend modifications</p> <p>Step 1: Clone and setup environment <pre><code># Clone the repository\ngit clone git@github.com:urban-toolkit/curio.git\ncd curio\n\n# Create conda environment (recommended)\nconda create -n curio python=3.10\nconda activate curio\n</code></pre></p> <p>Step 2: Install dependencies <pre><code># Install Python requirements\npip install -r requirements.txt\n\n# Install Node.js\nconda install -c conda-forge nodejs=22.13.0\n</code></pre></p> <p>Step 3: Start Curio <pre><code># Start all components\npython curio.py start\n\n# Or start individual servers\npython curio.py start backend\npython curio.py start sandbox\npython curio.py start frontend\n</code></pre></p> <p>Additional options: <pre><code># Force rebuild the frontend\npython curio.py start --force-rebuild\n\n# Force re-initialize the backend database\npython curio.py start --force-db-init\n</code></pre></p> <p>First Run</p> <p>The first time Curio runs, it will automatically install UTK. The installation of all required packages might take a few minutes.</p>"},{"location":"getting-started/installation/#ray-tracing-support","title":"\ud83d\uddbc\ufe0f Ray Tracing Support","text":"<p>To use Ray Tracing capabilities, please see UTK's requirements.</p>"},{"location":"getting-started/installation/#next-steps","title":"\u27a1\ufe0f Next Steps","text":"<p>Congratulations! You've successfully installed Curio. Here's what to do next:</p> <p>Get Started</p> <p>Follow our Quick Start Tutorial for a hands-on introduction to Curio's capabilities.</p> <p>Additional Resources: - \ud83d\udd27 User Guide - Detailed documentation - \ud83d\udcac Discord Community - Get help and connect with users - \ud83d\udc1b Report Issues - Found a bug?  </p> <p>Ready to build amazing urban analytics workflows? Get started with a Quick Start Tutorial! \ud83c\udf89</p>"},{"location":"getting-started/quick_start/","title":"\ud83d\udc5f Quick Start Tutorial","text":"<p>This tutorial will guide you through creating a Vega-Lite barchart visualization using the dataflow interface. We'll connect nodes to visualize a simple dataset, demonstrating Curio's core functionality.</p> <p>Before You Begin</p> <p>Make sure you have Curio installed and running. If you haven't installed Curio yet, check out our Installation Guide.</p>"},{"location":"getting-started/quick_start/#getting-started","title":"\ud83c\udf31 Getting Started","text":""},{"location":"getting-started/quick_start/#step-1-launch-curio","title":"Step 1: Launch Curio","text":"<p>After installation, start Curio and open your browser to access the interface. You'll see a blank canvas ready for building your first dataflow.</p>"},{"location":"getting-started/quick_start/#step-2-understanding-the-blank-canvas","title":"Step 2: Understanding the Blank Canvas","text":"<p>When you first open Curio, you'll see a blank canvas like this:</p> <p></p> <p>The icons on the left-hand side can be used to instantiate different nodes, including visualization ones.</p>"},{"location":"getting-started/quick_start/#creating-a-barchart","title":"\ud83d\udcca Creating a Barchart","text":"<p>In this tutorial, we are going to learn how Curio can easily help with visualizing a simple dataset using a Vega-Lite barchart.</p>"},{"location":"getting-started/quick_start/#step-3-add-data-loading-node","title":"Step 3: Add Data Loading Node","text":"<p>Let's start by instantiating a Data Loading node:</p> <ol> <li>Drag the Data Loading icon from the left sidebar.</li> <li>Change the view to <code>Code</code>.</li> <li>Enter the following synthetic dataset:</li> </ol> <p><pre><code>import pandas as pd\n\nd = {'a': [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"], \n     'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]}\ndf = pd.DataFrame(data=d)\n\nreturn df\n</code></pre> 4. Hit run - The Python <code>return</code> will output <code>df</code> for the next node.</p> <p></p> <p>External Files</p> <p>External files can be referenced through regular Python file handling functions, given that the file was uploaded to the server. For simplicity, we're using synthetic data here.</p>"},{"location":"getting-started/quick_start/#step-4-add-vega-lite-visualization-node","title":"Step 4: Add Vega-Lite Visualization Node","text":"<p>Now we'll create the visualization:</p> <ol> <li>Drag a <code>Vega-Lite</code> node.</li> <li>Connect it to the Data Loading node.</li> <li>Switch to the <code>Grammar</code> view and enter the Vega-Lite specification.</li> </ol> <p><pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"mark\": \"bar\",\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n</code></pre> 4. Hit run</p> <p></p> <p>Data Flow Magic</p> <p>Curio handles the data-flow automatically - Vega-Lite has access to the DataFrame outputted by the previous node.</p>"},{"location":"getting-started/quick_start/#step-5-see-your-results","title":"Step 5: See Your Results","text":"<p>After hitting run, you can see that a barchart was created:</p> <p></p> <p>Congratulations!</p> <p>You created your first data-flow using Curio! \ud83c\udf89</p>"},{"location":"getting-started/quick_start/#what-youve-learned","title":"\ud83d\udd0d What You've Learned","text":"<p>In this tutorial, you've learned:</p> <ul> <li>Node Creation - How to instantiate different types of nodes.</li> <li>Code vs Grammar Views - Different ways to configure nodes.</li> <li>Data Flow - How data passes between connected nodes.</li> <li>Vega-Lite Integration - Creating visualizations with grammar of graphics.</li> </ul>"},{"location":"getting-started/quick_start/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>Congratulations! You've created your first Curio workflow. Here's what to explore next:</p> <p>Continue Learning</p> <ul> <li>Explore Examples: Check out pre-built workflows in the examples gallery.</li> <li>Try Advanced Features: Experiment with UTK integration and interactive dashboards.</li> <li>Build Complex Workflows: Connect multiple nodes for urban analysis pipelines.</li> </ul>"},{"location":"getting-started/quick_start/#common-use-cases","title":"Common Use Cases","text":"<p>Curio excels at:</p> <ul> <li>Visual Analytics - Integrating heterogeneous urban datasets with interactive visualizations.</li> <li>Scenario Planning - What-if analysis and real-time urban development simulations.</li> <li>Machine Learning Workflows - Human-in-the-loop model training and evaluation.</li> <li>Energy &amp; Environment - Building efficiency analysis and green infrastructure mapping.</li> <li>Interactive Dashboards - Multi-view coordinated exploration with UTK and Vega-Lite.</li> </ul>"},{"location":"getting-started/quick_start/#need-help","title":"\ud83c\udd98 Need Help?","text":"<p>If you encounter any issues:</p> <ol> <li>Check Node Outputs - Look for error messages in node execution results.</li> <li>Validate Connections - Ensure data types match between connected nodes.</li> <li>Community Support - Ask questions on Discord</li> <li>Report Issues - Open an issue on GitHub</li> </ol> <p>Ready to dive deeper? Explore our curated Examples for advanced capabilities of Curio! \ud83d\ude80</p>"},{"location":"user-guide/overview/","title":"\ud83c\udf07 Welcome to the <code>Curio</code> User Guide!","text":"<p>In this guide, we will walk you through (1) the overall architecture of <code>Curio</code>, (2) <code>Curio</code> components and (3) framework modules and capabilities. Note that this guide does not cover all the features and capabilities of <code>Curio</code>, but it provides a good overview of what you can do with it.</p> <p>This guide demonstrates hands-on how to use <code>Curio</code> for collaborative urban visual analytics using dataflow-based workflows.</p>"},{"location":"user-guide/overview/#architecture-collaborative-visual-analytics","title":"\ud83c\udfd9\ufe0f Architecture: Collaborative Visual Analytics","text":"<p><code>Curio</code> uses a dataflow model with multiple abstraction levels  to facilitate collaboration across the design and implementation of visual analytics components. Think of it as a collaborative  workspace where urban experts, data scientists, and analysts can work together seamlessly.</p>"},{"location":"user-guide/overview/#dataflow-model-the-core-approach","title":"\ud83d\udd04 Dataflow Model: The Core Approach","text":"<p><code>Curio</code> organizes urban analysis into connected dataflow pipelines where:</p> <ul> <li>Multiple abstraction levels allow different users to contribute at their comfort level\u2014from code to GUI interactions.</li> <li>Visual dataflow graphs make complex analysis pipelines transparent and collaborative.</li> <li>Provenance tracking keeps track of how data flows through transformations and visualizations.</li> <li>Real-time collaboration enables teams to work together on the same analysis workflows.</li> </ul>"},{"location":"user-guide/overview/#components-the-building-blocks","title":"\ud83d\udee0\ufe0f Components: The Building Blocks","text":"<ul> <li>Data Loading Node: Import and prepare your urban datasets from various sources.</li> <li>Analysis &amp; Modeling Nodes: Transform, compute, and analyze your data using custom code or pre-built functions.</li> <li>Data Cleaning Node: Filter, clean, and prepare data for analysis.</li> <li>Visualization Nodes: Create interactive maps, charts, and plots using Vega-Lite, UTK, and other tools.</li> <li>Data Pool Node: Store and manage intermediate results for reuse across the workflow.</li> <li>Merge Node: Combine multiple data streams and coordinate complex workflows.</li> </ul>"},{"location":"user-guide/overview/#core-system-components-the-technical-foundation","title":"\ud83c\udfaf Core System Components: The Technical Foundation","text":"<ul> <li>Backend: Manages database access, user authentication, and system coordination</li> <li>Sandbox: Provides secure execution environment for user Python code and data processing</li> <li>Frontend: The user interface system with two main components:</li> <li>Urban Workflows: Main Curio interface for creating and editing dataflow pipelines</li> <li>UTK Workflow: Embedded urban visualization toolkit for spatial analysis and mapping</li> </ul>"},{"location":"user-guide/overview/#integration-capabilities","title":"\ud83d\udd17 Integration Capabilities","text":"<p><code>Curio</code> connects with external tools and services to extend its analytical power:</p> <ul> <li>Interactive Visualizations: Rich, coordinated interactions between maps and charts</li> <li>UTK Integration: Advanced urban visualization and spatial analysis capabilities  </li> <li>Vega-Lite Support: Grammar-based statistical visualizations with interaction support</li> <li>External Data Sources: Connect to APIs, databases, and online urban data repositories</li> <li>Collaborative Features: Real-time shared workspaces and provenance tracking</li> </ul> <p>Dataflow vs. Traditional Analysis: What's the Difference?</p> <ul> <li>Traditional Analysis typically follows a linear, script-based approach where each step builds on the previous one, making collaboration and iteration challenging.</li> <li>Curio's Dataflow Model breaks analysis into connected, reusable nodes that can be developed independently, tested in isolation, and combined in flexible ways. This makes it easier for teams to collaborate, experiment with different approaches, and maintain complex analytical workflows.</li> </ul> <p>Learning Through Examples</p> <p>This user guide focuses on <code>Curio</code>'s core concepts and modules. To see these capabilities in action with real urban data and collaborative workflows, explore our comprehensive examples section.</p> <p>Quick Start Tutorial  View Examples </p>"},{"location":"user-guide/overview/#ready-to-explore-each-component","title":"\ud83d\ude80 Ready to Explore Each Component?","text":"<p>Dive into these sections to understand <code>Curio</code>'s technical architecture and capabilities:</p> <ul> <li>Backend: Database management, user authentication, and system coordination</li> <li>Sandbox: Secure Python code execution and data processing environment</li> <li>Urban Workflows: Main interface for dataflow creation and collaboration</li> <li>UTK Workflow: Embedded urban visualization and spatial analysis toolkit</li> </ul> <p>Coming Soon</p> <p>This user guide is actively being developed. More detailed content for each module will be added soon. In the meantime, explore our examples section for hands-on tutorials and real-world use cases.</p> <p>Want to Contribute?</p> <p>Help us improve Curio! Check out our Contributing Guide to learn how to report issues, suggest features, or contribute code to the project.</p> <p>Contributing Guide </p>"},{"location":"user-guide/modules/backend/","title":"Backend","text":"<p>Coming Soon</p> <p>This user guide is actively being developed. More detailed content for each module will be added soon. In the meantime, explore our examples section for hands-on tutorials and real-world use cases.</p>"},{"location":"user-guide/modules/sandbox/","title":"Sandbox","text":"<p>Coming Soon</p> <p>This user guide is actively being developed. More detailed content for each module will be added soon. In the meantime, explore our examples section for hands-on tutorials and real-world use cases.</p>"},{"location":"user-guide/modules/urban-workflows/","title":"Urban Workflows","text":"<p>Coming Soon</p> <p>This user guide is actively being developed. More detailed content for each module will be added soon. In the meantime, explore our examples section for hands-on tutorials and real-world use cases.</p>"},{"location":"user-guide/modules/utk-workflow/","title":"UTK Workflow","text":"<p>Coming Soon</p> <p>This user guide is actively being developed. More detailed content for each module will be added soon. In the meantime, explore our examples section for hands-on tutorials and real-world use cases.</p>"}]}